{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5e047a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ba2de81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저 및 모델 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "60233ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.concat([pd.read_csv(f\"./data/input/{i}.csv\") for i in range(1, 4)])\n",
    "target_data = pd.concat([pd.read_csv(f\"./data/target/{i}_target.csv\") for i in range(1, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "45229cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 크기: (3860, 12)\n",
      "타겟 데이터 크기: (3860, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"입력 데이터 크기:\", input_data.shape)\n",
    "print(\"타겟 데이터 크기:\", target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16e09f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec</th>\n",
       "      <th>msg_type</th>\n",
       "      <th>channel</th>\n",
       "      <th>note</th>\n",
       "      <th>velocity</th>\n",
       "      <th>dynamic</th>\n",
       "      <th>accent</th>\n",
       "      <th>count</th>\n",
       "      <th>main_vol</th>\n",
       "      <th>depth</th>\n",
       "      <th>pedal</th>\n",
       "      <th>pan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>['note_on', 'note_on']</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[81, 54]</td>\n",
       "      <td>[70, 72]</td>\n",
       "      <td>mp</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>['note_off', 'note_off', 'note_on', 'note_on']</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[81, 54, 80, 61]</td>\n",
       "      <td>[0, 0, 71, 88]</td>\n",
       "      <td>pp</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>17.4</td>\n",
       "      <td>['note_off']</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[53]</td>\n",
       "      <td>[64]</td>\n",
       "      <td>mp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>17.5</td>\n",
       "      <td>['note_off', 'note_on', 'note_on', 'note_on', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[74, 58, 70, 58, 70, 74]</td>\n",
       "      <td>[64, 100, 100, 50, 75, 64]</td>\n",
       "      <td>mf</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>17.6</td>\n",
       "      <td>['note_off']</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[58]</td>\n",
       "      <td>[64]</td>\n",
       "      <td>mp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>17.7</td>\n",
       "      <td>['note_off', 'note_on', 'note_on']</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[70, 69, 69]</td>\n",
       "      <td>[64, 100, 59]</td>\n",
       "      <td>mp</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>17.8</td>\n",
       "      <td>['note_off']</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[70]</td>\n",
       "      <td>[64]</td>\n",
       "      <td>mp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3860 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sec                                           msg_type  \\\n",
       "0     0.0                             ['note_on', 'note_on']   \n",
       "1     0.1                                                 []   \n",
       "2     0.2                                                 []   \n",
       "3     0.3                                                 []   \n",
       "4     0.4     ['note_off', 'note_off', 'note_on', 'note_on']   \n",
       "..    ...                                                ...   \n",
       "128  17.4                                       ['note_off']   \n",
       "129  17.5  ['note_off', 'note_on', 'note_on', 'note_on', ...   \n",
       "130  17.6                                       ['note_off']   \n",
       "131  17.7                 ['note_off', 'note_on', 'note_on']   \n",
       "132  17.8                                       ['note_off']   \n",
       "\n",
       "                channel                      note                    velocity  \\\n",
       "0                [0, 0]                  [81, 54]                    [70, 72]   \n",
       "1                    []                        []                          []   \n",
       "2                    []                        []                          []   \n",
       "3                    []                        []                          []   \n",
       "4          [0, 0, 0, 0]          [81, 54, 80, 61]              [0, 0, 71, 88]   \n",
       "..                  ...                       ...                         ...   \n",
       "128                 [0]                      [53]                        [64]   \n",
       "129  [0, 0, 0, 0, 0, 0]  [74, 58, 70, 58, 70, 74]  [64, 100, 100, 50, 75, 64]   \n",
       "130                 [0]                      [58]                        [64]   \n",
       "131           [0, 0, 0]              [70, 69, 69]               [64, 100, 59]   \n",
       "132                 [0]                      [70]                        [64]   \n",
       "\n",
       "    dynamic  accent  count  main_vol  depth pedal  pan  \n",
       "0        mp       0      2         0      0     0    0  \n",
       "1       NaN       0      0         0      0     0    0  \n",
       "2       NaN       0      0         0      0     0    0  \n",
       "3       NaN       0      0         0      0     0    0  \n",
       "4        pp       0      4         0      0     0    0  \n",
       "..      ...     ...    ...       ...    ...   ...  ...  \n",
       "128      mp       0      1         0      0     0    0  \n",
       "129      mf       0      6         0      0     0    0  \n",
       "130      mp       0      1         0      0     0    0  \n",
       "131      mp       0      3         0      0     0    0  \n",
       "132      mp       0      1         0      0     0    0  \n",
       "\n",
       "[3860 rows x 12 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5d93c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_data[['sec', 'note']]\n",
    "y = target_data['pitch_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4796b8a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[81, 54]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>[81, 54, 80, 61]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sec              note\n",
       "0  0.0          [81, 54]\n",
       "1  0.1                []\n",
       "2  0.2                []\n",
       "3  0.3                []\n",
       "4  0.4  [81, 54, 80, 61]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "151b3488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81, 54]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[81, 54, 80, 61]\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in X['note']:\n",
    "    print(i)\n",
    "    cnt += 1\n",
    "    if cnt == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "90537c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sec  note1  note2  note3  note4  note5  note6  note7  note8  note9\n",
      "0     0.0   81.0   54.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "1     0.1    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "2     0.2    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "3     0.3    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "4     0.4   81.0   54.0   80.0   61.0    NaN    NaN    NaN    NaN    NaN\n",
      "..    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...\n",
      "128  17.4   53.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "129  17.5   74.0   58.0   70.0   58.0   70.0   74.0    NaN    NaN    NaN\n",
      "130  17.6   58.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "131  17.7   70.0   69.0   69.0    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "132  17.8   70.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "\n",
      "[3860 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18540\\3646838318.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.drop('note', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 리스트 중 가장 많은 요소를 포함하는 리스트 찾기\n",
    "max_length = max(len(eval(note)) for note in X['note'] if note)\n",
    "max_note = [note for note in X['note'] if len(eval(note)) == max_length][0]\n",
    "\n",
    "# 새로운 열 생성\n",
    "for i in range(max_length):\n",
    "    column_name = f\"note{i+1}\"\n",
    "    \n",
    "    X[column_name] = [eval(note)[i] if len(eval(note)) > i else float('nan') for note in X['note']]\n",
    "\n",
    "X.drop('note', axis=1, inplace=True)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "212bf931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch_accuracy</th>\n",
       "      <th>dynamic_similarity</th>\n",
       "      <th>dynamic_change_consistency</th>\n",
       "      <th>accent_accuracy</th>\n",
       "      <th>octave_similarity</th>\n",
       "      <th>pedal_consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>22.4</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>22.4</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>22.4</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>22.4</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>22.4</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3860 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pitch_accuracy  dynamic_similarity  dynamic_change_consistency  \\\n",
       "0                45                  64                          66   \n",
       "1                45                  64                          66   \n",
       "2                45                  64                          66   \n",
       "3                45                  64                          66   \n",
       "4                45                  64                          66   \n",
       "..              ...                 ...                         ...   \n",
       "128               9                   8                           7   \n",
       "129               9                   8                           7   \n",
       "130               9                   8                           7   \n",
       "131               9                   8                           7   \n",
       "132               9                   8                           7   \n",
       "\n",
       "     accent_accuracy  octave_similarity  pedal_consistency  \n",
       "0               22.4               45.1                 10  \n",
       "1               22.4               45.1                 10  \n",
       "2               22.4               45.1                 10  \n",
       "3               22.4               45.1                 10  \n",
       "4               22.4               45.1                 10  \n",
       "..               ...                ...                ...  \n",
       "128              6.0                1.0                  0  \n",
       "129              6.0                1.0                  0  \n",
       "130              6.0                1.0                  0  \n",
       "131              6.0                1.0                  0  \n",
       "132              6.0                1.0                  0  \n",
       "\n",
       "[3860 rows x 6 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "07049bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\" \".join(map(str, row)) for row in X.values.tolist()]\n",
    "\n",
    "encoded_inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "72ff71fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1014,  1012,  ...,  1014,  5139,   102],\n",
       "        [  101,  1014,  1012,  ..., 16660, 16660,   102],\n",
       "        [  101,  1014,  1012,  ..., 16660, 16660,   102],\n",
       "        ...,\n",
       "        [  101,  2459,  1012,  ...,  1014, 16660,   102],\n",
       "        [  101,  2459,  1012,  ...,  1014,  6353,   102],\n",
       "        [  101,  2459,  1012,  ...,  1014, 16660,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "814ab7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45., 45., 45., ...,  9.,  9.,  9.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 데이터를 numpy 배열로 변환\n",
    "labels = np.array(y)\n",
    "labels = labels.astype(float)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 1223.9826615882043\n",
      "Epoch 2/5, Train Loss: 772.9866095824563\n",
      "Epoch 3/5, Train Loss: 579.6624398503279\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT 입력 데이터로 변환\n",
    "def preprocess_inputs(texts):\n",
    "    texts = [\" \".join(map(str, row)) for row in texts.values.tolist()]\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "X_train_encoded = preprocess_inputs(X_train)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_encoded['input_ids'], X_train_encoded['attention_mask'], y_train_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for input_ids, attention_mask, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits.squeeze()\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * len(input_ids)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "037c3434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch의 TensorDataset으로 데이터셋 생성\n",
    "dataset = TensorDataset(encoded_inputs['input_ids'], encoded_inputs['attention_mask'], torch.tensor(labels))\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d234451b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 모델에 입력하여 출력 얻기\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 역전파 수행\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    229\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    235\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# 입력 데이터를 float32로 변환\n",
    "input_ids = input_ids.to(torch.float32)\n",
    "attention_mask = attention_mask.to(torch.float32)\n",
    "labels = labels.to(torch.float32)\n",
    "\n",
    "# 모델에 입력하여 출력 얻기\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "\n",
    "# 역전파 수행\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4c483415",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    229\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    235\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch in train_dataloader:\n",
    "        # 입력 데이터를 float32로 변환\n",
    "        input_ids = input_ids.to(torch.float32)\n",
    "        attention_mask = attention_mask.to(torch.float32)\n",
    "        labels = labels.to(torch.float32)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "298785bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10619047284126282, 0.07352332025766373, 0.07455848157405853, 0.0604725182056427, 0.08230486512184143, 0.04258742928504944, 0.09172294288873672, 0.09101834148168564, 0.0836043506860733, 0.08519034832715988, 0.05572304129600525, 0.07290370762348175, 0.1097240149974823, 0.07779846340417862, 0.10226508229970932, 0.0708080530166626, 0.11875531077384949, 0.10054834187030792, 0.0952720195055008, 0.1209716871380806, 0.048945896327495575, 0.10943728685379028, 0.06383505463600159, 0.0546400249004364, 0.09711980819702148, 0.07022961974143982, 0.12165485322475433, 0.10603538900613785, 0.0964139774441719, 0.12941253185272217, 0.054124414920806885, 0.10433299839496613, 0.06876678019762039, 0.005816318094730377, 0.0736081451177597, 0.05473387986421585, 0.10257821530103683, 0.08627250790596008, 0.07365491986274719, 0.09530805796384811, 0.03553861379623413, 0.09289149940013885, 0.08287310600280762, 0.03533980995416641, 0.04355911165475845, 0.05124308168888092, 0.09820401668548584, 0.08218776434659958, 0.07410146296024323, 0.0945209488272667, 0.06218951940536499, 0.1102992445230484, 0.10125018656253815, 0.0611661896109581, 0.08871476352214813, 0.06062904745340347, 0.11646698415279388, 0.10046004503965378, 0.09418060630559921, 0.13098227977752686, 0.07397744059562683, 0.12158653885126114, 0.07621658593416214, 0.07460946589708328, 0.07740315049886703, 0.08565184473991394, 0.11298447847366333, 0.11184504628181458, 0.09613620489835739, 0.1095912903547287, 0.07392540574073792, 0.12259236723184586, 0.11133043467998505, 0.08034700155258179, 0.10843702405691147, 0.0947197675704956, 0.14485576748847961, 0.09214373677968979, 0.10766395181417465, 0.12266112864017487, 0.0723833218216896, 0.11519744247198105, 0.10279766470193863, 0.09097161889076233, 0.09015136957168579, 0.08192075788974762, 0.11087920516729355, 0.07138878852128983, 0.08213313668966293, 0.10242358595132828, 0.07149503380060196, 0.11808371543884277, 0.09689374268054962, 0.06831424683332443, 0.09440939873456955, 0.0822538435459137, 0.10363192856311798, 0.10405242443084717, 0.09553292393684387, 0.10210855305194855, 0.06512497365474701, 0.11063890159130096, 0.10239092260599136, 0.06384360790252686, 0.08133065700531006, 0.07838626205921173, 0.11258774250745773, 0.09769180417060852, 0.07814307510852814, 0.10073785483837128, 0.035830333828926086, 0.07129431515932083, 0.06269101053476334, 0.02289460599422455, 0.04843667149543762, 0.05273941159248352, 0.0909465104341507, 0.07117114961147308, 0.05551454424858093, 0.0741519033908844, 0.048157259821891785, 0.08536125719547272, 0.07531460374593735, 0.039022982120513916, 0.07378901541233063, 0.0606212243437767, 0.04151148349046707, 0.08461296558380127, 0.0659203752875328, 0.07173401862382889, 0.03800535202026367, 0.07266610860824585, 0.06444292515516281, 0.032574236392974854, 0.04391676187515259, 0.05898607522249222, 0.05539649724960327, 0.07007431983947754, 0.08191763609647751, 0.059173814952373505, 0.11359710991382599, 0.07554510235786438, 0.07257673889398575, 0.08888723701238632, 0.05274093151092529, 0.048688001930713654, 0.07809006422758102, 0.053251221776008606, 0.0504634827375412, 0.13219083845615387, 0.044207364320755005, 0.08086324483156204, 0.0465940460562706, 0.03012675791978836, 0.06289641559123993, 0.04983024299144745, 0.07386110723018646, 0.06307310611009598, 0.0505310595035553, 0.1254388689994812, 0.03405141830444336, 0.04607774317264557, 0.0595051571726799, 0.0728154182434082, 0.04422437399625778, 0.041838161647319794, 0.03222010284662247, 0.06146795302629471, 0.045453645288944244, 0.04619556665420532, 0.01518222689628601, 0.07234567403793335, 0.06456691771745682, 0.029934778809547424, 0.04788815975189209, 0.038714706897735596, 0.08023367822170258, 0.06473937630653381, 0.043337106704711914, 0.0744355320930481, 0.023855984210968018, 0.06508240848779678, 0.06496452540159225, 0.0224386528134346, 0.017252542078495026, 0.03702986240386963, 0.07861574739217758, 0.061773255467414856, 0.0430329293012619, 0.09102991968393326, 0.04243527352809906, 0.09026264399290085, 0.08401884138584137, 0.06058505177497864, 0.05818674713373184, 0.06626555323600769, 0.10079662501811981, 0.0861944928765297, 0.03755659610033035, 0.0628751814365387, 0.03227407485246658, 0.08218096196651459, 0.0773283839225769, 0.040381863713264465, 0.05451309680938721, 0.06506135314702988, 0.08933595567941666, 0.04828960448503494, 0.05880025029182434, 0.06065814942121506, 0.029108621180057526, 0.0697881281375885, 0.06295556575059891, 0.03071444481611252, 0.024887919425964355, 0.051023490726947784, 0.07999593019485474, 0.06464530527591705, 0.04707343876361847, 0.02402997761964798, 0.02966940402984619, 0.05808977037668228, 0.06447537988424301, 0.026071317493915558, 0.050574734807014465, 0.042169809341430664, 0.03846528381109238, 0.0589168444275856, 0.045644573867321014, 0.05009111016988754, 0.035043664276599884, 0.07593949139118195, 0.07476608455181122, 0.03562547266483307, 0.05870070308446884, -0.007126450538635254, 0.08867409080266953, 0.040384478867053986, 0.0500476211309433, 0.03402481973171234, 0.028972245752811432, 0.06635288894176483, 0.06072276830673218, 0.022151336073875427, 0.001477651298046112, 0.035228267312049866, 0.08073431253433228, 0.0695599690079689, 0.05124547332525253, 0.012565553188323975, 0.02825714647769928, 0.059914976358413696, 0.053145259618759155, 0.056248150765895844, 0.04003330320119858, 0.04358723759651184, 0.006272122263908386, 0.05909927934408188, 0.021069690585136414, 0.05132811516523361, 0.02594038099050522, 0.05859605222940445, 0.04813707619905472, 0.04437674582004547, 0.035250820219516754, 0.03396157920360565, 0.06701278686523438, 0.1048046424984932, 0.038389429450035095, 0.03787101060152054, 0.016077488660812378, 0.05486931651830673, 0.04621288180351257, 0.015262305736541748, 0.03775443136692047, 0.03962665796279907, 0.06543933600187302, 0.05090603977441788, 0.03134628385305405, 0.03189997375011444, 0.010799601674079895, 0.024518907070159912, 0.04084271192550659, 0.03135017305612564, 0.026545144617557526, 0.02277611941099167, 0.05832919478416443, 0.04201715439558029, 0.08996344357728958, 0.025123894214630127, 0.06452455371618271, 0.06819694489240646, 0.05778580904006958, 0.0419900044798851, 0.0386924147605896, 0.032339729368686676, 0.07243780046701431, 0.11673420667648315, 0.036886729300022125, 0.040682509541511536, 0.018880806863307953, 0.06747404485940933, 0.05397171527147293, 0.01804526150226593, 0.037729762494564056, 0.032011546194553375, 0.10146253556013107, 0.0601387694478035, 0.039929889142513275, 0.041772402822971344, 0.015709400177001953, 0.0619083046913147, 0.06391549855470657, 0.020126990973949432, 0.04485981911420822, 0.05135567486286163, 0.07249748706817627, 0.06037340313196182, -0.005517065525054932, 0.0461881160736084, 0.01986604928970337, 0.060109928250312805, 0.051312655210494995, 0.029928073287010193, 0.041515134274959564, 0.04200056940317154, 0.06615237146615982, 0.03252069652080536, 0.04721440374851227, 0.046208977699279785, 0.018161185085773468, 0.048481762409210205, -0.024093061685562134, 0.010736197233200073, 0.03399880230426788, 0.040768496692180634, 0.04117307811975479, 0.05147179216146469, 0.03402263671159744, 0.03621990233659744, 0.0290224626660347, 0.08271387964487076, 0.06196148693561554, 0.028156310319900513, 0.05015912652015686, 0.05303278565406799, 0.05390150845050812, 0.0619756281375885, 0.0705433264374733, 0.05135670304298401, 0.07305504381656647, 0.07166089117527008, 0.06726919114589691, 0.03368975222110748, 0.05138849467039108, 0.03839198499917984, 0.06264722347259521, 0.05545589327812195, 0.047501206398010254, 0.08649168908596039, 0.03624235838651657, 0.08168168365955353, 0.08592675626277924, 0.05014719069004059, 0.04487884044647217, 0.05878443270921707, 0.02871822565793991, 0.07457345724105835, 0.06055164337158203, 0.062355153262615204, 0.0554092600941658, 0.09719870984554291, 0.09809225052595139, 0.04026249051094055, 0.0852733850479126, 0.07840359210968018, 0.10098682343959808, 0.08020284026861191, 0.05914795398712158, 0.08848458528518677, 0.04045230150222778, 0.07189706712961197, 0.06615587323904037, 0.02759760618209839, 0.055974751710891724, 0.03883480280637741, 0.07072817534208298, 0.05426444113254547, 0.03677704930305481, 0.05229391157627106, 0.03992314636707306, 0.07906366884708405, 0.02452097088098526, 0.03766456991434097, 0.0560794398188591, 0.05870142579078674, 0.04492773115634918, 0.0637829378247261, 0.04948800057172775, 0.06431038677692413, 0.02608630061149597, 0.024382255971431732, 0.05685126781463623, 0.00501607358455658, 0.042270734906196594, 0.0380413681268692, 0.022466763854026794, 0.03769807517528534, 0.02833510935306549, 0.040364235639572144, 0.03667953610420227, 0.05943628400564194, 0.042576856911182404, 0.017962202429771423, 0.03606580197811127, 0.05687335878610611, 0.05363748222589493, 0.06097562611103058, 0.02049010992050171, 0.04811803996562958, 0.01756114512681961, 0.05258149653673172, 0.04921223223209381, 0.015607774257659912, 0.030883021652698517, 0.03863873332738876, 0.09355449676513672, 0.04199322313070297, 0.025271251797676086, 0.04389367252588272, 0.026594966650009155, 0.0848129391670227, 0.04922504723072052, 0.01468629390001297, 0.04418497532606125, 0.04872395098209381, 0.06525644659996033, 0.05158612132072449, 0.06786808371543884, 0.046206459403038025, 0.040973588824272156, 0.043552614748477936, 0.03563053160905838, 0.03942660987377167, 0.03700997680425644, 0.07383537292480469, 0.05178634822368622, 0.019225463271141052, 0.02218583971261978, 0.09541264921426773, 0.010351613163948059, 0.05412761867046356, 0.032220251858234406, 0.012111984193325043, 0.04757475107908249, 0.039102837443351746, 0.06304538995027542, 0.036996789276599884, 0.02249567210674286, 0.07649238407611847, 0.027967743575572968, 0.04495752602815628, 0.05943017452955246, 0.05929172784090042, 0.053392596542835236, 0.05650819092988968, 0.03749579191207886, 0.05657292157411575, 0.04173142462968826, 0.04658057540655136, 0.059224024415016174, 0.07770295441150665, 0.07088954746723175, 0.04734966903924942, 0.06158943474292755, 0.02668333798646927, 0.09424703568220139, 0.06757627427577972, 0.05789627134799957, 0.013161562383174896, 0.025409378111362457, 0.061560772359371185, 0.05093154311180115, 0.02557934820652008, 0.02484794706106186, 0.05780481547117233, 0.07656949013471603, 0.06624505668878555, 0.04721991717815399, 0.07512372732162476, 0.03270933777093887, 0.07423059642314911, 0.07304240018129349, 0.08048152923583984, 0.0572848916053772, 0.07278858125209808, 0.0928419679403305, 0.07745932787656784, 0.06783928722143173, 0.06846856325864792, 0.0910988599061966, 0.06848105788230896, 0.05456286668777466, 0.05234402418136597, 0.042418740689754486, 0.09430209547281265, 0.07084377110004425, 0.04025723785161972, 0.04485118389129639, 0.07158105820417404, 0.015953868627548218, 0.04968441277742386, 0.05936937779188156, 0.005616992712020874, 0.04723655432462692, 0.04713015258312225, 0.059956714510917664, 0.042301930487155914, 0.035552918910980225, 0.0910622850060463, 0.02401137351989746, 0.02261193096637726, 0.0415533185005188, 0.09318359941244125, 0.035540997982025146, 0.047172509133815765, 0.03551532328128815, 0.049118876457214355, 0.05177184194326401, 0.02550673484802246, 0.0740358904004097, 0.048414528369903564, 0.04913371801376343, 0.06947193294763565, 0.044383615255355835, 0.023668460547924042, 0.06279237568378448, 0.0744367241859436, 0.03396748751401901, 0.03235071152448654, 0.02802152931690216, 0.05452457070350647, 0.0273282527923584, 0.018440380692481995, 0.033304572105407715, 0.054268933832645416, 0.04385847598314285, 0.05560886859893799, 0.04833131283521652, 0.0343996062874794, 0.042678192257881165, 0.05978860706090927, 0.07621274888515472, 0.002851061522960663, 0.06627961248159409, 0.06781355291604996, 0.08701323717832565, 0.07152750343084335, 0.05194483697414398, 0.07019694894552231, 0.04103023558855057, 0.0708199143409729, 0.07017891108989716, 0.07115522027015686, 0.06461252272129059, 0.03042510151863098, 0.0775763988494873, 0.017644047737121582, 0.05405193567276001, 0.05726809799671173, 0.04383823275566101, 0.0640612542629242, 0.03239460289478302, 0.04701937735080719, 0.08422322571277618, 0.07468433678150177, 0.04584495723247528, 0.0680844634771347, 0.05855298042297363, 0.10652334243059158, 0.05163536220788956, 0.060552842915058136, 0.08978603780269623, 0.05262060463428497, 0.07553045451641083, 0.07858826220035553, 0.09116705507040024, 0.08035273104906082, 0.10553386062383652, 0.06963613629341125, 0.07170634716749191, 0.06950752437114716, 0.0696290135383606, 0.10131333768367767, 0.05706228315830231, 0.037345945835113525, 0.07838492095470428, 0.11968199908733368, 0.04897026717662811, 0.06678865849971771, 0.042593494057655334, 0.07891108840703964, 0.07287340611219406, 0.06740661710500717, 0.039780013263225555, 0.0581948384642601, 0.08480054140090942, 0.06607828289270401, 0.05947905778884888, 0.035749152302742004, 0.033000290393829346, 0.06462984532117844, 0.05971357226371765, 0.023550137877464294, 0.05093332380056381, 0.05278486758470535, 0.059628985822200775, 0.05808943510055542, 0.05530901253223419, 0.048986390233039856, 0.05054408311843872, 0.07068073004484177, 0.06316017359495163, 0.03876020759344101, 0.05884021520614624, 0.0739402323961258, 0.07484988868236542, 0.018793143332004547, 0.04219081997871399, 0.052886322140693665, 0.050992876291275024, 0.06815065443515778, 0.045832179486751556, 0.034421294927597046, 0.05768529325723648, 0.06256397813558578, 0.06941095739603043, 0.06577983498573303, 0.04969102889299393, 0.05693993717432022, 0.05831955373287201, 0.05980020761489868, 0.06016679108142853, 0.08006218820810318, 0.05075778812170029, 0.050801314413547516, 0.057603687047958374, 0.05857575684785843, 0.09957658499479294, 0.047055572271347046, 0.03723198175430298, 0.09359140694141388, 0.08636347204446793, 0.08406438678503036, 0.07368031144142151, 0.042463384568691254, 0.0952693447470665, 0.07894477993249893, 0.07531343400478363, 0.07812050729990005, 0.04587056487798691, 0.07962186634540558, 0.07822170853614807, 0.03990267217159271, 0.036821119487285614, 0.06867913156747818, 0.09590931236743927, 0.07346874475479126, 0.055243007838726044, 0.02351071685552597, 0.03376050293445587, 0.08821743726730347, 0.052749454975128174, 0.03702995926141739, 0.04766983538866043, 0.05126962810754776, 0.07365716993808746, 0.05502747744321823, 0.054767854511737823, 0.057500600814819336, 0.08142893016338348, 0.06405479460954666, 0.06602389365434647, 0.046086184680461884, 0.05195885896682739, 0.06967011839151382, 0.062101855874061584, 0.036250777542591095, 0.04695376753807068, 0.06600635498762131, 0.02976149320602417, 0.0646367073059082, 0.058635540306568146, 0.025198601186275482, 0.04862470179796219, 0.059507817029953, 0.08329756557941437, 0.06029443442821503, 0.04695610702037811, 0.07755818217992783, 0.021067380905151367, -0.017102569341659546, 0.043814755976200104, 0.005100876092910767, 0.03690969944000244, 0.03265588730573654, 0.04769275337457657, 0.043566107749938965, 0.03192613273859024, 0.033498987555503845, 0.007953830063343048, 0.039995305240154266, 0.028189629316329956, -0.005614966154098511, 0.033395230770111084, 0.022586464881896973, 0.046252958476543427, 0.02554706484079361, 0.012960381805896759, 0.015415020287036896, 0.016645073890686035, 0.03306466341018677, 0.02465447038412094, -0.0002819374203681946, 0.026899777352809906, 0.02410396933555603, 0.04375021159648895, 0.030130289494991302, 0.012623324990272522, 0.01636851578950882, 0.021822266280651093, 0.04692079871892929, 0.04018445312976837, 0.008085429668426514, 0.03552603721618652, 0.04042069613933563, 0.05926358699798584, 0.047412797808647156, 0.02985331416130066, 0.030676841735839844, 0.03409987688064575, 0.05719921737909317, 0.0572718009352684, 0.022876054048538208, 0.05495544523000717, 0.0571976900100708, 0.07490468770265579, 0.060003213584423065, 0.04427375644445419, 0.044477276504039764, 0.02786443382501602, 0.06275913119316101, 0.05102228373289108, 0.007784724235534668, 0.04366983473300934, 0.04566193372011185, 0.07174214720726013, 0.05753079801797867, 0.040063560009002686, 0.03992060571908951, 0.01924872398376465, 0.048151642084121704, 0.04561053216457367, 0.012669861316680908, 0.04188194125890732, 0.04216916114091873, 0.06285098195075989, 0.043698281049728394, 0.036361366510391235, 0.03799763321876526, 0.029412373900413513, 0.06212373077869415, 0.05418819934129715, 0.02339344471693039, 0.04881419986486435, 0.05718883126974106, 0.07550348341464996, 0.06430936604738235, 0.05096205323934555, 0.047138512134552, 0.04439065605401993, 0.07614363729953766, 0.057331688702106476, 0.03915543109178543, 0.06654421985149384, 0.0644683763384819, 0.08159112185239792, 0.07552089542150497, 0.05915398895740509, 0.06423914432525635, 0.06464428454637527, 0.09163776785135269, 0.0744691789150238, 0.034015171229839325, 0.06351219862699509, 0.0650969073176384, 0.08473444730043411, 0.07070045918226242, 0.06101839989423752, 0.07035932689905167, 0.03301094472408295, 0.06724520027637482, 0.06453204900026321, 0.022517316043376923, 0.05373823642730713, 0.05325879901647568, 0.07771000266075134, 0.06699841469526291, 0.06296718865633011, 0.06392902880907059, 0.04965800791978836, 0.07166004180908203, 0.06326279044151306, 0.031873784959316254, 0.05792643874883652, 0.06040099263191223, 0.0859517753124237, 0.06660342216491699, 0.05911416560411453, 0.05856943130493164, 0.036665767431259155, 0.05384329706430435, 0.05069826543331146, 0.02438635379076004, 0.049542710185050964, 0.05267201364040375, 0.0720881000161171, 0.05602918565273285, 0.05270420014858246, 0.0563327819108963, 0.035861216485500336, 0.05953556299209595, 0.05359528958797455, 0.020895935595035553, 0.05601469427347183, 0.05278349667787552, 0.07054658979177475, 0.049664050340652466, 0.04815030097961426, 0.05358751863241196, 0.018153995275497437, 0.04217247664928436, 0.035119615495204926, 0.009991168975830078, 0.03961227834224701, 0.03832682967185974, 0.04912327975034714, 0.03711067885160446, 0.03331885486841202, 0.030692726373672485, 0.04828071594238281, 0.06462696194648743, 0.06229700148105621, 0.02995534986257553, 0.055908918380737305, 0.06170963495969772, 0.07290063053369522, 0.06312280893325806, 0.05652488023042679, 0.058151647448539734, 0.06911814212799072, 0.08674831688404083, 0.08748109638690948, 0.049158692359924316, 0.08308430761098862, 0.08271406590938568, 0.09445870667695999, 0.07954145222902298, 0.07206566631793976, 0.0805523544549942, 0.048971883952617645, 0.07405832409858704, 0.06852559745311737, 0.029834233224391937, 0.061576902866363525, 0.06477592885494232, 0.07885701954364777, 0.06262049078941345, 0.05493415147066116, 0.05797683447599411, 0.04103277623653412, 0.07645969837903976, 0.06987447291612625, 0.038548544049263, 0.06209355592727661, 0.057928211987018585, 0.07699908316135406, 0.058397695422172546, 0.05539817363023758, 0.05588170886039734, 0.04245023429393768, 0.07085973024368286, 0.07205110788345337, 0.04038786143064499, 0.06633903086185455, 0.06364549696445465, 0.08474335819482803, 0.051616571843624115, 0.05744476616382599, 0.06441961228847504, 0.0381958931684494, 0.07552192360162735, 0.07209356129169464, 0.03323880583047867, 0.06224512308835983, 0.05247585475444794, 0.07452964782714844, 0.044490694999694824, 0.04443079233169556, 0.06079947203397751, 0.0442599281668663, 0.061794258654117584, 0.05890718847513199, 0.022915631532669067, 0.04843570291996002, 0.04545566439628601, 0.06728001683950424, 0.02731228619813919, 0.02238055318593979, 0.03826291859149933, 0.032503873109817505, 0.05089039355516434, 0.05304396152496338, 0.02772308886051178, 0.05093254894018173, 0.048199042677879333, 0.06754794716835022, 0.03496480733156204, 0.03317667543888092, 0.04327879846096039, 0.044294506311416626, 0.0536462664604187, 0.0653117448091507, 0.032423101365566254, 0.04907654970884323, 0.059440262615680695, 0.07664071768522263, 0.04779709875583649, 0.03757640719413757, 0.04495939612388611, 0.054180607199668884, 0.06348125636577606, 0.07289955019950867, 0.03582015633583069, 0.06651005148887634, 0.06127018481492996, 0.08520127087831497, 0.05516165494918823, 0.04428296536207199, 0.05476325750350952, 0.03562803566455841, 0.07068619877099991, 0.08104255050420761, 0.04200209677219391, 0.0678420290350914, 0.06576330214738846, 0.08821640908718109, 0.06316416710615158, 0.06135028600692749, 0.06267235428094864, 0.04125269502401352, 0.05531621724367142, 0.062253788113594055, 0.03385051339864731, 0.05751277506351471, 0.057597070932388306, 0.07588263601064682, 0.05298808217048645, 0.0490289181470871, 0.047636210918426514, 0.03863796591758728, 0.060411982238292694, 0.06177078187465668, 0.03706912696361542, 0.05755738914012909, 0.061726562678813934, 0.08286695927381516, 0.05833867937326431, 0.055100783705711365, 0.06048882007598877, 0.02890104055404663, 0.04772050678730011, 0.045398272573947906, 0.02883777767419815, 0.04985237866640091, 0.041984736919403076, 0.06290746480226517, 0.03429831564426422, 0.030262067914009094, 0.038967281579971313, 0.022333204746246338, 0.06281810998916626, 0.058612748980522156, 0.03012106567621231, 0.0567849725484848, 0.058210067451000214, 0.08227396011352539, 0.060479044914245605, 0.05199097841978073, 0.06490535289049149, 0.027458980679512024, 0.051949143409729004, 0.05369400233030319, 0.021531857550144196, 0.05201760679483414, 0.04874908924102783, 0.07465657591819763, 0.05344465374946594, 0.03871224820613861, 0.055782467126846313, 0.03612549602985382, 0.07098887115716934, 0.07692478597164154, 0.04109295457601547, 0.060520581901073456, 0.06360931694507599, 0.09203431755304337, 0.06281496584415436, 0.05706062912940979, 0.0713701918721199, 0.030425146222114563, 0.058513008058071136, 0.06437119096517563, 0.03038553148508072, 0.05807173252105713, 0.05977853387594223, 0.07689428329467773, 0.05694673955440521, 0.051511093974113464, 0.05753868818283081, 0.037937089800834656, 0.05312106013298035, 0.06029466539621353, 0.03607048839330673, 0.05747406184673309, 0.0617498978972435, 0.07281988114118576, 0.053667113184928894, 0.055788978934288025, 0.05229003727436066, 0.046480365097522736, 0.05754699558019638, 0.06871762871742249, 0.039047226309776306, 0.057909123599529266, 0.06710465252399445, 0.07591847330331802, 0.06239160895347595, 0.0566699281334877, 0.05088813602924347, 0.05514979362487793, 0.07857251167297363, 0.0765361338853836, 0.04704535752534866, 0.0696391835808754, 0.0773470401763916, 0.08218906819820404, 0.05901721864938736, 0.06252104789018631, 0.06051485240459442, 0.04268277436494827, 0.0633464828133583, 0.06659433990716934, 0.036088623106479645, 0.062280088663101196, 0.06941631436347961, 0.05026548355817795, 0.04947651922702789, 0.04475776106119156, 0.04444262385368347, 0.04832883179187775, 0.06481507420539856, 0.05991334468126297, 0.03350722789764404, 0.060049280524253845, 0.06945618987083435, 0.067811518907547, 0.05223923921585083, 0.04865260422229767, 0.04459841549396515, 0.04733048379421234, 0.07308321446180344, 0.05543254315853119, 0.03325732797384262, 0.053997308015823364, 0.06465979665517807, 0.07590611279010773, 0.07292306423187256, 0.05296565592288971, 0.06454294919967651, 0.05461737513542175, 0.08334004878997803, 0.08744321763515472, 0.04314207285642624, 0.06511852890253067, 0.07756920158863068, 0.08540251106023788, 0.0799083262681961, 0.06921181827783585, 0.0753609836101532, 0.04140160232782364, 0.06276517361402512, 0.06546828895807266, 0.019189126789569855, 0.05069176107645035, 0.05694414675235748, 0.06956477463245392, 0.06634701788425446, 0.05725955218076706, 0.06344030052423477, 0.05116162449121475, 0.07722803950309753, 0.08057958632707596, 0.04416472464799881, 0.0688764750957489, 0.07856925576925278, 0.08841623365879059, 0.08064190298318863, 0.07270526885986328, 0.07398998737335205, 0.0269162580370903, 0.05874112248420715, 0.0681178867816925, 0.030446209013462067, 0.05831989645957947, 0.06202799081802368, 0.07897315174341202, 0.06355578452348709, 0.053919970989227295, 0.055422425270080566, 0.0504876971244812, 0.06833325326442719, 0.08169589936733246, 0.04325230419635773, 0.07468678057193756, 0.07887368649244308, 0.09637539088726044, 0.07786013185977936, 0.06010901927947998, 0.06627918034791946, 0.03746534138917923, 0.05921158939599991, 0.05823265761137009, 0.016902461647987366, 0.05400247871875763, 0.06026502698659897, 0.07668548822402954, 0.06179853528738022, 0.040302492678165436, 0.04678385704755783, 0.04001802206039429, 0.07862471044063568, 0.0646546334028244, 0.0185893252491951, 0.05436848849058151, 0.06483891606330872, 0.07843954861164093, 0.06928453594446182, 0.05147010087966919, 0.052692629396915436, 0.04393577575683594, 0.07977419346570969, 0.07155251502990723, 0.025409609079360962, 0.055705100297927856, 0.06875026226043701, 0.0806574672460556, 0.073667973279953, 0.05755259096622467, 0.05212978273630142, 0.05325620621442795, 0.08152449131011963, 0.09296640753746033, 0.05569511651992798, 0.07784152776002884, 0.09418325126171112, 0.10608169436454773, 0.0924539640545845, 0.08338922262191772, 0.07890480756759644, 0.049445487558841705, 0.07650481164455414, 0.08635877817869186, 0.04977107793092728, 0.07074921578168869, 0.08207462728023529, 0.09086298942565918, 0.08087581396102905, 0.0648571103811264, 0.06344009190797806, 0.06821093708276749, 0.0897899866104126, 0.08915238082408905, 0.05017582327127457, 0.07551263272762299, 0.0891844630241394, 0.10243730247020721, 0.09113883972167969, 0.07702476531267166, 0.07833490520715714, 0.0522700697183609, 0.0822310820221901, 0.08225534856319427, 0.0433117151260376, 0.06851910054683685, 0.07720939815044403, 0.09364860504865646, 0.0851391851902008, 0.07073219865560532, 0.07115014642477036, 0.04074223339557648, 0.07331806421279907, 0.07060820609331131, 0.03295958787202835, 0.0581723228096962, 0.07314763963222504, 0.0936146229505539, 0.07103684544563293, 0.06319917738437653, 0.0609007328748703, 0.04597049206495285, 0.05780470371246338, 0.07010256499052048, 0.03594154119491577, 0.058309972286224365, 0.07426556199789047, 0.09084916114807129, 0.07818695157766342, 0.06308164447546005, 0.05768798291683197, 0.04338432103395462, 0.06687398254871368, 0.06716014444828033, 0.031632378697395325, 0.05809023231267929, 0.073441281914711, 0.09044141322374344, 0.0725792944431305, 0.06928657740354538, 0.06212075054645538, 0.04948627948760986, 0.06978590786457062, 0.07226906716823578, 0.033340856432914734, 0.056196197867393494, 0.07184172421693802, 0.08280037343502045, 0.07426628470420837, 0.05948321521282196, 0.055282652378082275, 0.05794690549373627, 0.0840342715382576, 0.07923760265111923, 0.03704412281513214, 0.06763093173503876, 0.07861816138029099, 0.09505773335695267, 0.09123332053422928, 0.07059944421052933, 0.07470833510160446, 0.03525432199239731, 0.060695916414260864, 0.052705541253089905, 0.0197109654545784, 0.047414906322956085, 0.060419611632823944, 0.06938982754945755, 0.05892207473516464, 0.04950425773859024, 0.04802919179201126, 0.03553426265716553, 0.06800679117441177, 0.06456473469734192, 0.021267086267471313, 0.04650784283876419, 0.0626346617937088, 0.08164796978235245, 0.0666036456823349, 0.05330827087163925, 0.055384889245033264, 0.03351576626300812, 0.06278304755687714, 0.06159145385026932, 0.02547573298215866, 0.04821861535310745, 0.0576908215880394, 0.08048830926418304, 0.07036693394184113, 0.05781445652246475, 0.05478368699550629, 0.024560965597629547, 0.0488109216094017, 0.06162996590137482, 0.018910475075244904, 0.04679155349731445, 0.06145692616701126, 0.080629363656044, 0.06754398345947266, 0.058766648173332214, 0.047162480652332306, 0.032560452818870544, 0.051882483065128326, 0.05271513760089874, 0.012760557234287262, 0.04371701925992966, 0.049229949712753296, 0.06271178275346756, 0.053816162049770355, 0.04605129361152649, 0.045352160930633545, 0.03929615020751953, 0.06956068426370621, 0.06522485613822937, 0.022960707545280457, 0.05197470635175705, 0.05788753926753998, 0.08302362263202667, 0.0659821629524231, 0.06189785897731781, 0.05284678190946579, 0.02714044600725174, 0.035397037863731384, 0.051370084285736084, 0.012482456862926483, 0.03650520741939545, 0.0503292977809906, 0.067908376455307, 0.05729033052921295, 0.04151694476604462, 0.0298699289560318, 0.05196795612573624, 0.067599818110466, 0.06308092176914215, 0.027673475444316864, 0.05273696780204773, 0.06467891484498978, 0.07873615622520447, 0.071982242166996, 0.053931549191474915, 0.05696704238653183, 0.04003842920064926, 0.045107245445251465, 0.0579477921128273, 0.024622485041618347, 0.054455459117889404, 0.06409072875976562, 0.07519646733999252, 0.060561440885066986, 0.04493491351604462, 0.038766101002693176, 0.04526503384113312, 0.07475405186414719, 0.0622696727514267, 0.027952417731285095, 0.056967467069625854, 0.06206003576517105, 0.06779395788908005, 0.06454148888587952, 0.05768689513206482, 0.06375740468502045, 0.0363895446062088, 0.05555536597967148, 0.057463377714157104, 0.01990956813097, 0.051513299345970154, 0.06196167320013046, 0.07570907473564148, 0.06424396485090256, 0.05589914321899414, 0.04159209132194519, 0.04378426820039749, 0.05291485786437988, 0.05698263645172119, 0.014891490340232849, 0.04827132076025009, 0.0615159273147583, 0.06263791769742966, 0.05856804549694061, 0.04823014885187149, 0.03748365491628647, 0.0626215785741806, 0.07430125772953033, 0.08431456983089447, 0.05055224150419235, 0.07769501209259033, 0.09017828851938248, 0.10034330934286118, 0.09089107811450958, 0.07452109456062317, 0.07119917124509811, 0.05114962160587311, 0.06771659106016159, 0.0708414763212204, 0.028962701559066772, 0.06021235138177872, 0.07467189431190491, 0.08914296329021454, 0.08343349397182465, 0.0689915269613266, 0.0629904493689537, 0.037706196308135986, 0.06200099736452103, 0.06356529146432877, 0.0187017023563385, 0.05351579189300537, 0.05944143980741501, 0.07505204528570175, 0.06743419915437698, 0.05517220497131348, 0.05780038982629776, 0.039344049990177155, 0.06657937169075012, 0.06397297233343124, 0.028205804526805878, 0.058697521686553955, 0.0625390112400055, 0.0827144905924797, 0.07171094417572021, 0.06567806005477905, 0.05880768597126007, 0.036574169993400574, 0.06241772323846817, 0.07280310243368149, 0.03226882964372635, 0.059369318187236786, 0.06788890063762665, 0.08042198419570923, 0.07536210864782333, 0.06276701390743256, 0.054485417902469635, 0.04047069698572159, 0.05784009397029877, 0.07670006155967712, 0.0330948680639267, 0.059377193450927734, 0.06760543584823608, 0.07927802950143814, 0.07303822040557861, 0.061851613223552704, 0.06329894810914993, 0.034513235092163086, 0.07017196714878082, 0.07461382448673248, 0.031239688396453857, 0.05680382251739502, 0.05864476412534714, 0.08020702004432678, 0.07177982479333878, 0.06558717787265778, 0.06580023467540741, 0.030875027179718018, 0.07157271355390549, 0.07670479267835617, 0.026890181005001068, 0.058162152767181396, 0.06823215633630753, 0.07977331429719925, 0.06646615266799927, 0.056409627199172974, 0.05616120994091034, 0.05101519823074341, 0.07892778515815735, 0.08356839418411255, 0.03992534428834915, 0.0698123425245285, 0.07560531049966812, 0.09543800354003906, 0.0806022435426712, 0.06662842631340027, 0.0690637081861496, 0.03813616931438446, 0.0734892264008522, 0.0788174495100975, 0.03206796944141388, 0.062387704849243164, 0.06988462805747986, 0.08269305527210236, 0.08267125487327576, 0.06352812796831131, 0.05975193530321121, 0.024728700518608093, 0.046748071908950806, 0.060738734900951385, 0.014752469956874847, 0.0446406826376915, 0.0552162304520607, 0.0625811517238617, 0.05702865868806839, 0.03412772715091705, 0.037101343274116516, 0.030112311244010925, 0.05847357213497162, 0.07143847644329071, 0.017482668161392212, 0.04972446709871292, 0.057272493839263916, 0.07310006767511368, 0.05805901437997818, 0.03816857188940048, 0.041531361639499664, 0.04486919194459915, 0.06296728551387787, 0.07394290715456009, 0.028657451272010803, 0.058620885014534, 0.057278990745544434, 0.07910878956317902, 0.07016373425722122, 0.05141385644674301, 0.05857647955417633, 0.02320259064435959, 0.051771581172943115, 0.0570983961224556, 0.009428754448890686, 0.03938739746809006, 0.04721786081790924, 0.06776678562164307, 0.04821081459522247, 0.039914317429065704, 0.0403766930103302, 0.03247174620628357, 0.0417758971452713, 0.05617103725671768, 0.007658824324607849, 0.035402655601501465, 0.049973875284194946, 0.05927165597677231, 0.05146755278110504, 0.03315676003694534, 0.032912738621234894, 0.04152558743953705, 0.051952578127384186, 0.06572337448596954, 0.020025886595249176, 0.05108312517404556, 0.06627225875854492, 0.07751766592264175, 0.06301933526992798, 0.0539095401763916, 0.049984097480773926, 0.028523266315460205, 0.04143306612968445, 0.06016181409358978, 0.01084601879119873, 0.04414525628089905, 0.05050063878297806, 0.0676790177822113, 0.05469062179327011, 0.04511319100856781, 0.0437215119600296, 0.04483720660209656, 0.060051366686820984, 0.07223427295684814, 0.030143462121486664, 0.059312038123607635, 0.06634269654750824, 0.07448528707027435, 0.07265862822532654, 0.06460709124803543, 0.059221141040325165, 0.040960878133773804, 0.06709711998701096, 0.08135844022035599, 0.03700628876686096, 0.06478052586317062, 0.07289335876703262, 0.07752851396799088, 0.08509511500597, 0.07163888961076736, 0.06288927793502808, 0.053403474390506744, 0.0845041424036026, 0.08370258659124374, 0.04422114044427872, 0.07529342919588089, 0.07822297513484955, 0.09126506745815277, 0.08582055568695068, 0.0842592865228653, 0.08208715915679932, 0.032839082181453705, 0.06235212832689285, 0.06792273372411728, 0.03069636970758438, 0.05988363176584244, 0.06499306112527847, 0.07509534806013107, 0.06630735099315643, 0.05573270469903946, 0.05174214392900467, 0.057103224098682404, 0.08733408153057098, 0.09498508274555206, 0.05132373422384262, 0.08299248665571213, 0.08983898907899857, 0.09961723536252975, 0.09120256453752518, 0.08339536190032959, 0.08300577104091644, 0.04860885441303253, 0.07864924520254135, 0.08430749177932739, 0.03592022508382797, 0.0732603371143341, 0.06849653273820877, 0.08660757541656494, 0.07829222828149796, 0.07308702915906906, 0.043564170598983765, 0.007284760475158691, 0.042576782405376434, 0.041041769087314606, 0.020959921181201935, 0.08908438682556152, 0.07347384840250015, 0.04592869430780411, 0.06881795823574066, 0.09530588984489441, 0.031211301684379578, 0.03664758801460266, 0.07063183188438416, 0.10554438829421997, 0.05612746626138687, 0.08377106487751007, 0.036929771304130554, 0.0019152015447616577, 0.028366610407829285, 0.06588025391101837, -0.004032760858535767, 0.07031047344207764, 0.07629675418138504, 0.0638774111866951, 0.06157226860523224, 0.0429919958114624, 0.10102354735136032, 0.07239839434623718, 0.08902259916067123, 0.048276640474796295, 0.07212614268064499, 0.04025430977344513, 0.029441997408866882, 0.04780382663011551, 0.03590843081474304, 0.0487712100148201, -0.026879750192165375, 0.06239372491836548, 0.032933786511421204, 0.03608659654855728, -0.008240215480327606, 0.048020139336586, 0.028690680861473083, 0.04889601469039917, 0.029156342148780823, 0.013723917305469513, -0.004626244306564331, 0.05946555733680725, 0.05040980130434036, 0.0301906019449234, 0.08261997997760773, 0.0862932875752449, 0.0512789711356163, 0.03489504009485245, 0.05225541442632675, 0.08136960864067078, 0.10195180773735046, 0.05662775784730911, 0.049547113478183746, 0.026378057897090912, 0.11476434022188187, 0.04034843295812607, 0.051832184195518494, 0.03846549987792969, 0.09554256498813629, 0.04823379963636398, 0.06365043669939041, 0.03658642619848251, 0.0018337741494178772, 0.04241568595170975, 0.04958868771791458, -0.031081005930900574, 0.057537756860256195, 0.09109844267368317, 0.08364249765872955, 0.1016557589173317, 0.053853169083595276, 0.034891627728939056, 0.09695945680141449, 0.060430437326431274, 0.0598672479391098, 0.08253047615289688, 0.045859500765800476, 0.0360441654920578, 0.05279167741537094, 0.0660632997751236, 0.030913040041923523, -0.024849899113178253, 0.05169194936752319, 0.0345257893204689, 0.05146327614784241, 0.008264683187007904, 0.04960212856531143, 0.05014711618423462, 0.059134140610694885, 0.039801083505153656, -0.0015986040234565735, 0.008569642901420593, 0.045975372195243835, 0.04394977539777756, 0.05908548831939697, 0.04877498000860214, 0.09618811309337616, 0.0862957090139389, 0.04455497860908508, 0.023778431117534637, 0.09993692487478256, 0.05348128825426102, 0.08088890463113785, 0.04012833535671234, 0.02628219872713089, 0.07794427126646042, 0.057783693075180054, 0.07290861010551453, 0.043379999697208405, 0.0809503048658371, 0.04205283522605896, 0.08939506858587265, 0.052021175622940063, 0.016115352511405945, 0.06708262860774994, 0.07503104209899902, 0.07940927892923355, 0.027993924915790558, 0.053097158670425415, 0.023705042898654938, 0.037508271634578705, 0.04855731129646301, 0.02674204111099243, 0.0483023002743721, 0.057703666388988495, -8.188188076019287e-06, 0.039743006229400635, 0.024244070053100586, 0.05655620992183685, 0.06632804870605469, 0.019120246171951294, 0.05267991125583649, 0.06208834797143936, 0.09166885167360306, 0.06795860826969147, 0.09521487355232239, 0.034549519419670105, 0.09001120924949646, 0.06131690740585327, 0.0408167839050293, 0.05383766442537308, 0.053268857300281525, 0.0897265300154686, 0.08304041624069214, 0.11978142708539963, 0.03255583345890045, 0.09092505276203156, 0.06649669259786606, -0.00035019218921661377, 0.06186965852975845, 0.08071605116128922, 0.0673586055636406, 0.04144681990146637, 0.08821354806423187, 0.039359599351882935, 0.02299153059720993, 0.04977439343929291, 0.047483883798122406, 0.0716240257024765, 0.023225858807563782, -0.019578948616981506, 0.042958229780197144, 0.02281711995601654, 0.028216347098350525, 0.037639811635017395, 0.020161673426628113, 0.08580177277326584, 0.08741234242916107, 0.07561436295509338, 0.04289411008358002, 0.09203185141086578, 0.05584285408258438, 0.1006791815161705, 0.03972020745277405, 0.042682945728302, 0.05781599134206772, 0.059323012828826904, 0.05113862454891205, 0.08931510895490646, 0.04433618485927582, 0.11344673484563828, 0.06868825852870941, 0.05727633833885193, 0.04854847490787506, 0.06914231181144714, 0.033637531101703644, 0.04631775617599487, 0.017081402242183685, 0.036965496838092804, 0.06525124609470367, 0.0016855373978614807, 0.0502779483795166, 0.04359305649995804, 0.00048603862524032593, 0.036624230444431305, 0.04768200218677521, 0.008586004376411438, 0.048044353723526, 0.050870686769485474, 0.07566089183092117, 0.06501753628253937, 0.0894022062420845, 0.013927116990089417, 0.09218810498714447, 0.03473760932683945, 0.022902749478816986, 0.0643962174654007, 0.07082480192184448, 0.05467841774225235, 0.042314812541007996, 0.0975056067109108, 0.041156552731990814, 0.09035328030586243, 0.05852131545543671, 0.03034977614879608, 0.08358211070299149, 0.05565996468067169, 0.0579279363155365, 0.03421242535114288, 0.06352915614843369, 0.03226177394390106, 0.03797007352113724, 0.03924433887004852, 0.02707602083683014, 0.02614252269268036, 0.04931129515171051, -0.029220521450042725, 0.03380279242992401, 0.01891772449016571, 0.02087387442588806, 0.057177864015102386, 0.025115802884101868, 0.03589184582233429, 0.04426020383834839, 0.05993843078613281, 0.056796520948410034, 0.08297324925661087, 0.04740598797798157, 0.05229242891073227, 0.016049250960350037, 0.027594678103923798, 0.06413978338241577, 0.025766678154468536, 0.07333748787641525, 0.044212207198143005, 0.03044193983078003, 0.04061523824930191, 0.059770017862319946, 0.10496193915605545, 0.05090486258268356, 0.05499088019132614, 0.03764750808477402, 0.051053546369075775, 0.0274493545293808, 0.022119037806987762, 0.027025356888771057, 0.012379243969917297, 0.04568278789520264, 0.045449309051036835, 0.08460336923599243, 0.047023169696331024, 0.0594615563750267, 0.06433502584695816, 0.05021355301141739, -0.004923820495605469, 0.05567218363285065, 0.02729051560163498, 0.061197370290756226, 0.03866908699274063, 0.09001044929027557, 0.048821620643138885, 0.03682754933834076, 0.029462002217769623, 0.03266770392656326, 0.03514230251312256, 0.07534760236740112, 0.025844141840934753, 0.02521038055419922, 0.02387310564517975, 0.040961652994155884, 0.028011709451675415, 0.021315477788448334, -0.004425540566444397, 0.0009647458791732788, -0.009735524654388428, 0.028722800314426422, -0.004973292350769043, 0.03490544110536575, 0.002981528639793396, 0.03834380954504013, 0.055759407579898834, 0.011287547647953033, 0.06704869121313095, 0.062282949686050415, 0.06825166940689087, 0.05293866991996765, 0.07983923703432083, 0.07570710778236389, 0.03100595623254776, 0.060550376772880554, 0.06828196346759796, 0.09498969465494156, 0.09120507538318634, 0.07136882096529007, 0.07517553120851517, 0.01564723253250122, 0.034464552998542786, 0.035701312124729156, 0.0017170235514640808, 0.03293579816818237, 0.03624865412712097, 0.07737325131893158, 0.06424212455749512, 0.04125283658504486, 0.040255747735500336, 0.038618966937065125, 0.061783142387866974, 0.06697607040405273, 0.030563771724700928, 0.05572749674320221, 0.06128382682800293, 0.08276515454053879, 0.07886983454227448, 0.06892784684896469, 0.06887129694223404, 0.01760997623205185, 0.048662155866622925, 0.05395886301994324, 0.009467348456382751, 0.034129440784454346, 0.033999256789684296, 0.06800977140665054, 0.05885482579469681, 0.04543308913707733, 0.051539093255996704, 0.039716996252536774, 0.06954006105661392, 0.06913217902183533, 0.01766500622034073, 0.052368611097335815, 0.053386494517326355, 0.08207482844591141, 0.0755661129951477, 0.0618072971701622, 0.06523924320936203, 0.033762916922569275, 0.06301172822713852, 0.05248401314020157, 0.013077527284622192, 0.04261447489261627, 0.053688280284404755, 0.0800994336605072, 0.06835338473320007, 0.06102624535560608, 0.06498183310031891, 0.03915299475193024, 0.07644842565059662, 0.06299164146184921, 0.026219487190246582, 0.05625122785568237, 0.06479468941688538, 0.08205929398536682, 0.07704515755176544, 0.07248270511627197, 0.07453605532646179, 0.024446293711662292, 0.05191360414028168, 0.04275590181350708, 0.00848899781703949, 0.03701291233301163, 0.04189479351043701, 0.06729240715503693, 0.05862344801425934, 0.05194906145334244, 0.05606289952993393, 0.02289988100528717, 0.06437395513057709, 0.06503365188837051, 0.027751095592975616, 0.055932171642780304, 0.06363679468631744, 0.08602215349674225, 0.07804247736930847, 0.06756085902452469, 0.07327234745025635, 0.03514920175075531, 0.05654585361480713, 0.062202706933021545, 0.01861373335123062, 0.053503721952438354, 0.04179216921329498, 0.08540531247854233, 0.0685301274061203, 0.047751426696777344, 0.07186522334814072, 0.03411237895488739, 0.05674641579389572, 0.05473130941390991, 0.016799159348011017, 0.046151429414749146, 0.05842771381139755, 0.08415623754262924, 0.0666089653968811, 0.05380113422870636, 0.05956355482339859, 0.018003419041633606, 0.04520312696695328, 0.05415799468755722, 0.02377648651599884, 0.045357756316661835, 0.05309247970581055, 0.0745471715927124, 0.05128397047519684, 0.043671734631061554, 0.04139330983161926, 0.04540769010782242, 0.06889151781797409, 0.07790480554103851, 0.04068954288959503, 0.06740752607584, 0.0841534212231636, 0.10207733511924744, 0.08495212346315384, 0.07721519470214844, 0.07863187789916992, 0.037954285740852356, 0.04805012047290802, 0.05388554930686951, 0.014137029647827148, 0.04812197387218475, 0.05380738526582718, 0.08905286341905594, 0.0717296302318573, 0.05662359297275543, 0.06737973541021347, 0.05210339277982712, 0.07293634116649628, 0.0790567547082901, 0.031434446573257446, 0.06626474857330322, 0.07472068816423416, 0.10729160159826279, 0.08977821469306946, 0.08420251309871674, 0.08442634344100952, 0.02741210162639618, 0.047670021653175354, 0.05296803265810013, 0.02326875925064087, 0.050790876150131226, 0.05593021214008331, 0.08276000618934631, 0.06435811519622803, 0.06054163724184036, 0.05571608245372772, 0.055687934160232544, 0.07895708084106445, 0.07674583792686462, 0.042155057191848755, 0.0660414770245552, 0.07863567769527435, 0.10688843578100204, 0.09807703644037247, 0.0860079973936081, 0.08634671568870544, 0.03642028570175171, 0.06480563431978226, 0.056765079498291016, 0.009809218347072601, 0.046117186546325684, 0.04129788279533386, 0.07562500238418579, 0.07021507620811462, 0.06209046393632889, 0.0700351819396019, 0.05593495070934296, 0.07736409455537796, 0.06886103004217148, 0.035151779651641846, 0.06119348853826523, 0.06955699622631073, 0.10850580036640167, 0.09803081303834915, 0.08001390099525452, 0.09333187341690063, 0.04929562658071518, 0.05981086939573288, 0.05511043220758438, 0.015667006373405457, 0.0460614413022995, 0.04954221844673157, 0.07818202674388885, 0.07535101473331451, 0.06819189339876175, 0.07327169924974442, 0.040212228894233704, 0.0675240233540535, 0.0641881451010704, 0.02785121649503708, 0.0554572269320488, 0.06273791193962097, 0.0997966006398201, 0.08770141750574112, 0.06835555285215378, 0.06889639794826508, 0.045461125671863556, 0.056917592883110046, 0.05380566418170929, 0.018254362046718597, 0.0439065545797348, 0.049075305461883545, 0.08671710640192032, 0.08292141556739807, 0.06347840279340744, 0.057410188019275665, 0.051354750990867615, 0.06409966200590134, 0.05477762222290039, 0.025137677788734436, 0.05260833352804184, 0.0556785985827446, 0.08533111959695816, 0.0729173868894577, 0.05402349680662155, 0.06000039726495743, 0.05508024990558624, 0.08021783828735352, 0.08662208169698715, 0.05058283358812332, 0.07807205617427826, 0.08041109144687653, 0.11146616190671921, 0.09922467917203903, 0.08605363219976425, 0.09008780866861343, 0.04116303473711014, 0.062100060284137726, 0.05423816293478012, 0.016549408435821533, 0.04866079241037369, 0.053132593631744385, 0.08598794043064117, 0.07483432441949844, 0.06151831895112991, 0.0696088895201683, 0.03286159038543701, 0.05511751025915146, 0.039724744856357574, 0.009586349129676819, 0.03874334692955017, 0.0333060622215271, 0.07419878244400024, 0.061847299337387085, 0.03820923715829849, 0.05918170511722565, 0.04305458813905716, 0.08676977455615997, 0.07356970012187958, 0.03565842658281326, 0.06676498055458069, 0.07022534310817719, 0.10536860674619675, 0.09343753755092621, 0.07407775521278381, 0.08896872401237488, 0.0613512322306633, 0.07773086428642273, 0.07337326556444168, 0.04031626135110855, 0.07557574659585953, 0.07691572606563568, 0.10347813367843628, 0.09831780940294266, 0.08239302784204483, 0.08098907768726349, 0.046128615736961365, 0.0763884037733078, 0.07143159210681915, 0.023482546210289, 0.06027330458164215, 0.06905155628919601, 0.09332482516765594, 0.09199594706296921, 0.07514612376689911, 0.07845129072666168, 0.057081520557403564, 0.06613457202911377, 0.07866745442152023, 0.030760809779167175, 0.06754086166620255, 0.06367216259241104, 0.1011788547039032, 0.09337574988603592, 0.07860346883535385, 0.09613712131977081, 0.04629868268966675, 0.07624638080596924, 0.07325664162635803, 0.03615830838680267, 0.06527338922023773, 0.07405991852283478, 0.10609673708677292, 0.08893585950136185, 0.07182399928569794, 0.07869584113359451, 0.05920384079217911, 0.06743159145116806, 0.07346832752227783, 0.04442223161458969, 0.06581408530473709, 0.0733359158039093, 0.10173017531633377, 0.09084374457597733, 0.07317627221345901, 0.07793960720300674, 0.05030953884124756, 0.06661862134933472, 0.06791478395462036, 0.026411183178424835, 0.05149732530117035, 0.05878636986017227, 0.0902588814496994, 0.08223366737365723, 0.06182399392127991, 0.07380063086748123, 0.056807368993759155, 0.07156290113925934, 0.07758283615112305, 0.03223828971385956, 0.05846460163593292, 0.06840749084949493, 0.11150100827217102, 0.10944880545139313, 0.07819758355617523, 0.09228161722421646, 0.07143383473157883, 0.08214718848466873, 0.10020546615123749, 0.05031327158212662, 0.0800420418381691, 0.08732984960079193, 0.12147942185401917, 0.11358078569173813, 0.09393201768398285, 0.10680367797613144, 0.0861227884888649, 0.08856132626533508, 0.09562622010707855, 0.05392460525035858, 0.08402998000383377, 0.08931493759155273, 0.1255166232585907, 0.11111914366483688, 0.09490492194890976, 0.10001707077026367, 0.062038227915763855, 0.0721091479063034, 0.08709659427404404, 0.05734536796808243, 0.07607544958591461, 0.07776318490505219, 0.11579301208257675, 0.11247135698795319, 0.0782957598567009, 0.0937640517950058, 0.05065371096134186, 0.08308917284011841, 0.0905366986989975, 0.05009469389915466, 0.07357889413833618, 0.07119818031787872, 0.09946199506521225, 0.09604109823703766, 0.08811968564987183, 0.10391610115766525, 0.05570293962955475, 0.07826592773199081, 0.08035161346197128, 0.03603333979845047, 0.06874894350767136, 0.06656336039304733, 0.10979384183883667, 0.09780697524547577, 0.08492302894592285, 0.09920859336853027, 0.052615612745285034, 0.08081822842359543, 0.09142483025789261, 0.05488207936286926, 0.07981885224580765, 0.0734972208738327, 0.10239709913730621, 0.09746979922056198, 0.08683088421821594, 0.10444828122854233, 0.042307280004024506, 0.07262309640645981, 0.07559854537248611, 0.03223210573196411, 0.05980411171913147, 0.051174916326999664, 0.09585246443748474, 0.07495956122875214, 0.05196486413478851, 0.07435527443885803, 0.04278732091188431, 0.06710060685873032, 0.07376299798488617, 0.04269777983427048, 0.06641222536563873, 0.0731629878282547, 0.10862401872873306, 0.09329897165298462, 0.07834228128194809, 0.08816617727279663, 0.04461515694856644, 0.06591786444187164, 0.07587401568889618, 0.03560272604227066, 0.06422220170497894, 0.07064009457826614, 0.09902133047580719, 0.08039822429418564, 0.06777332723140717, 0.08389713615179062, 0.057180970907211304, 0.10133334994316101, 0.09376170486211777, 0.0532323494553566, 0.08386806398630142, 0.08842692524194717, 0.1259784698486328, 0.11123157292604446, 0.1022418811917305, 0.11959405243396759, 0.07345680147409439, 0.09017274528741837, 0.09844278544187546, 0.05111716687679291, 0.08235399425029755, 0.08824361860752106, 0.11564436554908752, 0.10034539550542831, 0.08901529014110565, 0.10727820545434952, 0.0501822754740715, 0.0854547917842865, 0.09304682910442352, 0.0468190461397171, 0.07720163464546204, 0.07567258179187775, 0.11065342277288437, 0.09690221399068832, 0.07850931584835052, 0.09176088869571686, 0.07055943459272385, 0.10145434737205505, 0.11149340867996216, 0.0653756707906723, 0.09059321135282516, 0.10067909955978394, 0.131367027759552, 0.1191578358411789, 0.10374058783054352, 0.11041197180747986, 0.07134274393320084, 0.05274004489183426, 0.09577108919620514, 0.0553482249379158, 0.08333049714565277, 0.0887225940823555, 0.11019465327262878, 0.1069512739777565, 0.07621920108795166, 0.08573096245527267, 0.055491164326667786, 0.08687728643417358, 0.0942772924900055, 0.050432443618774414, 0.07638783752918243, 0.08208512514829636, 0.11162781715393066, 0.10187271237373352, 0.08794821798801422, 0.08854780346155167, 0.04038792848587036, 0.0647183433175087, 0.0717361718416214, 0.032711632549762726, 0.056875571608543396, 0.05233018845319748, 0.09313605725765228, 0.08103064447641373, 0.06938264518976212, 0.08757726848125458, 0.06736156344413757, 0.08502303808927536, 0.09748538583517075, 0.05297321081161499, 0.07390130311250687, 0.07635289430618286, 0.11376053839921951, 0.10177759826183319, 0.08351770043373108, 0.09362944960594177, 0.04974835366010666, 0.0713796466588974, 0.08074065297842026, 0.04051417112350464, 0.06904750317335129, 0.06855398416519165, 0.10703570395708084, 0.08819817006587982, 0.05952940881252289, 0.072877898812294, 0.048777222633361816, 0.06985298544168472, 0.08257995545864105, 0.046934545040130615, 0.07000374794006348, 0.06634341925382614, 0.10738200694322586, 0.09190507233142853, 0.07506929337978363, 0.0859161913394928, 0.03314411640167236, 0.05777673423290253, 0.07233293354511261, 0.030456915497779846, 0.061304762959480286, 0.060868166387081146, 0.10061868280172348, 0.08151515573263168, 0.06227827072143555, 0.07868749648332596, 0.07414399087429047, 0.1023474857211113, 0.10886058211326599, 0.06771770864725113, 0.08946346491575241, 0.0964835062623024, 0.12893739342689514, 0.11134316027164459, 0.10566622018814087, 0.11211902648210526, 0.047567158937454224, 0.07141976058483124, 0.0841539204120636, 0.043493568897247314, 0.07122834771871567, 0.07120344042778015, 0.0969143807888031, 0.08688763529062271, 0.07851297408342361, 0.08268917351961136, 0.04986102879047394, 0.08176647871732712, 0.08924081176519394, 0.04617542028427124, 0.07310957461595535, 0.07875515520572662, 0.11775591969490051, 0.1072961688041687, 0.09901817888021469, 0.1103534996509552, 0.03032340109348297, 0.06800246238708496, 0.07957935333251953, 0.03408317267894745, 0.0651334673166275, 0.05514645576477051, 0.09497225284576416, 0.08670846372842789, 0.07295471429824829, 0.08771183341741562, 0.035232216119766235, 0.07101865857839584, 0.07323256134986877, 0.03335844725370407, 0.05974370241165161, 0.06620010733604431, 0.09426112473011017, 0.081519715487957, 0.07698763906955719, 0.07778529822826385, 0.03435724973678589, 0.060672178864479065, 0.06863009184598923, 0.02868000417947769, 0.0608660951256752, 0.05462093651294708, 0.09259983897209167, 0.08944191783666611, 0.06183343380689621, 0.08662348985671997, 0.033934131264686584, 0.06555526703596115, 0.08751684427261353, 0.03795493394136429, 0.06648077070713043, 0.07365088164806366, 0.10239283740520477, 0.08953051269054413, 0.07836257666349411, 0.07850012183189392, 0.04194723069667816, 0.07604788988828659, 0.08736167103052139, 0.0502193421125412, 0.06695245206356049, 0.08203094452619553, 0.10633605718612671, 0.08866124600172043, 0.07627053558826447, 0.07822544872760773, 0.06295235455036163, 0.08705609291791916, 0.09660487622022629, 0.055625975131988525, 0.08069904148578644, 0.08999738842248917, 0.11122302711009979, 0.10271225869655609, 0.09224057197570801, 0.09503644704818726, 0.03441407531499863, 0.04822841286659241, 0.0335550457239151, 0.0057279616594314575, 0.04195243865251541, 0.043455034494400024, 0.07211000472307205, 0.06236160546541214, 0.03993044048547745, 0.05638895183801651, 0.03459038585424423, 0.07621325552463531, 0.07311785221099854, 0.037300124764442444, 0.06199470907449722, 0.07495039701461792, 0.1122693121433258, 0.09564362466335297, 0.08028887212276459, 0.08098287135362625, 0.016511380672454834, 0.02666928619146347, 0.025647468864917755, 0.009545445442199707, 0.016645677387714386, 0.034356728196144104, 0.06758709251880646, 0.05876503884792328, 0.04461979866027832, 0.06240229308605194, 0.0512302964925766, 0.08392835408449173, 0.06992574036121368, 0.03753411024808884, 0.06854187697172165, 0.07179044932126999, 0.10234611481428146, 0.0958913117647171, 0.08078411221504211, 0.0938316211104393, 0.07373892515897751, 0.09220704436302185, 0.08567793667316437, 0.05378264933824539, 0.08474171161651611, 0.08854735642671585, 0.10529196262359619, 0.1055101677775383, 0.08925927430391312, 0.10258635878562927, 0.055299945175647736, 0.07410664111375809, 0.08017537742853165, 0.04194243252277374, 0.07205583900213242, 0.06577078998088837, 0.10176106542348862, 0.08254119008779526, 0.06629972159862518, 0.07756632566452026, 0.04946645349264145, 0.06079351156949997, 0.06076215207576752, 0.023468434810638428, 0.04930475354194641, 0.04321723431348801, 0.07967349141836166, 0.06848232448101044, 0.04650659114122391, 0.07202104479074478, 0.06787139177322388, 0.08485652506351471, 0.08558294177055359, 0.045652687549591064, 0.07563707232475281, 0.07861164957284927, 0.11193131655454636, 0.10024810582399368, 0.08674333989620209, 0.0992879644036293, 0.04364754259586334, 0.0586334764957428, 0.06165419518947601, 0.02897655963897705, 0.052660055458545685, 0.059027984738349915, 0.08062044531106949, 0.07371402531862259, 0.05285640060901642, 0.06545420736074448, 0.05105169862508774, 0.07399135828018188, 0.07717503607273102, 0.039934031665325165, 0.06481584161520004, 0.07201892882585526, 0.0997781977057457, 0.09054545313119888, 0.06756381690502167, 0.07435362040996552, 0.0060316771268844604, 0.040348052978515625, 0.044295117259025574, 0.011893078684806824, 0.03443780541419983, 0.03715050220489502, 0.06888041645288467, 0.06397010385990143, 0.04990978538990021, 0.05834915488958359, 0.041627295315265656, 0.07305852323770523, 0.07909959554672241, 0.029922254383563995, 0.06196516007184982, 0.05917762964963913, 0.08375627547502518, 0.07292388379573822, 0.06311830133199692, 0.0700162947177887, 0.04964515566825867, 0.05458369106054306, 0.061717696487903595, 0.022419467568397522, 0.05107849836349487, 0.05133138597011566, 0.08301609754562378, 0.07834641635417938, 0.06284244358539581, 0.06256738305091858, 0.054978713393211365, 0.06407258659601212, 0.07196307927370071, 0.02937351167201996, 0.06068456172943115, 0.05978301167488098, 0.09357784688472748, 0.08590290695428848, 0.07695091515779495, 0.08425351977348328, 0.0683818981051445, 0.07955177128314972, 0.08036918938159943, 0.046531885862350464, 0.07272374629974365, 0.07911239564418793, 0.1016017273068428, 0.10054370760917664, 0.09022082388401031, 0.0904872938990593, 0.05645529925823212, 0.06562571227550507, 0.07691776752471924, 0.030393287539482117, 0.05920582264661789, 0.06519392877817154, 0.09220311790704727, 0.09054253995418549, 0.07662687450647354, 0.0839693620800972, 0.024492532014846802, 0.0455789640545845, 0.05304236710071564, 0.015906423330307007, 0.044016093015670776, 0.03807307034730911, 0.07824531197547913, 0.0659833624958992, 0.0462445393204689, 0.057625457644462585, 0.04559527337551117, 0.06453090161085129, 0.07110154628753662, 0.0319163054227829, 0.06137222796678543, 0.0630263090133667, 0.09626772254705429, 0.08437041938304901, 0.07064893841743469, 0.07223719358444214, 0.05965080112218857, 0.08152222633361816, 0.08881621062755585, 0.0581081360578537, 0.08483336120843887, 0.0853218138217926, 0.11741766333580017, 0.10628277063369751, 0.08797754347324371, 0.09150471538305283, 0.06733579188585281, 0.07332191616296768, 0.08387595415115356, 0.04876915365457535, 0.07850342243909836, 0.07818378508090973, 0.11181627959012985, 0.10245965421199799, 0.08321060240268707, 0.09193707257509232, 0.0604834109544754, 0.08319158852100372, 0.08041173219680786, 0.04870904982089996, 0.0830242931842804, 0.07892384380102158, 0.10428601503372192, 0.09639256447553635, 0.08382853865623474, 0.09095641225576401, 0.049616582691669464, 0.06414946168661118, 0.052562303841114044, 0.019324898719787598, 0.053498171269893646, 0.057628557085990906, 0.0899297446012497, 0.07863160967826843, 0.05994541198015213, 0.07178692519664764, 0.03874623775482178, 0.0661502331495285, 0.04969984292984009, 0.015677809715270996, 0.04992132633924484, 0.05131909251213074, 0.07969223707914352, 0.07492875307798386, 0.06430857628583908, 0.07519908249378204, 0.03989598900079727, 0.0779149979352951, 0.07320722937583923, 0.033555954694747925, 0.06099022924900055, 0.07124048471450806, 0.09193309396505356, 0.07965180277824402, 0.06670760363340378, 0.06603045016527176, 0.05019109696149826, 0.0841856300830841, 0.08209910243749619, 0.04038280248641968, 0.07562117278575897, 0.07526331394910812, 0.10120177268981934, 0.08960073441267014, 0.0789482444524765, 0.07825285196304321, 0.04600100964307785, 0.06704798340797424, 0.06555411964654922, 0.02705763280391693, 0.05864328145980835, 0.06016124039888382, 0.08637535572052002, 0.07162825763225555, 0.06286516785621643, 0.07506248354911804, 0.0380350723862648, 0.0564318522810936, 0.058656491339206696, 0.01732565462589264, 0.05124061554670334, 0.044773489236831665, 0.07903863489627838, 0.06510550528764725, 0.05342072993516922, 0.06617658585309982, 0.0748935341835022, 0.08622477948665619, 0.09692699462175369, 0.05553458631038666, 0.08935220539569855, 0.08863015472888947, 0.12629608809947968, 0.09788601100444794, 0.08787406980991364, 0.09546775370836258, 0.050437651574611664, 0.0624149888753891, 0.06948667019605637, 0.029482707381248474, 0.06444685161113739, 0.05388853698968887, 0.09474452584981918, 0.08244816213846207, 0.06038346141576767, 0.06660681962966919, 0.050612904131412506, 0.05674146115779877, 0.06129513680934906, 0.024591021239757538, 0.05499940365552902, 0.06469611823558807, 0.10463747382164001, 0.09177841246128082, 0.07470151036977768, 0.07273197174072266, 0.04596830904483795, 0.06879493594169617, 0.0836746096611023, 0.029631711542606354, 0.06416235864162445, 0.06615756452083588, 0.10267870128154755, 0.09444160014390945, 0.08116879314184189, 0.08248069137334824, 0.052041538059711456, 0.06167702376842499, 0.07015160471200943, 0.026269428431987762, 0.05307028442621231, 0.056281231343746185, 0.08630464226007462, 0.08618436008691788, 0.06666691601276398, 0.07199978083372116, 0.02725622057914734, 0.049364298582077026, 0.053467556834220886, 0.01988694816827774, 0.04916410148143768, 0.05029929429292679, 0.0835784301161766, 0.07545122504234314, 0.05619043856859207, 0.06389064341783524, 0.04450993984937668, 0.07251450419425964, 0.074744313955307, 0.033879347145557404, 0.06901217997074127, 0.07491718977689743, 0.10863938927650452, 0.09166043996810913, 0.07556261867284775, 0.07802663743495941, 0.025781869888305664, 0.06339774280786514, 0.058266304433345795, 0.022943027317523956, 0.05637073516845703, 0.060644909739494324, 0.08619356155395508, 0.06979785859584808, 0.06471306085586548, 0.07363239675760269, 0.03532180190086365, 0.06636582314968109, 0.08217242360115051, 0.03826518356800079, 0.07099181413650513, 0.06749556958675385, 0.10807599872350693, 0.08640133589506149, 0.0692916139960289, 0.08052544295787811, 0.05031341314315796, 0.07559631764888763, 0.07713806629180908, 0.029969803988933563, 0.062318310141563416, 0.05848504602909088, 0.10274537652730942, 0.08463305979967117, 0.06664391607046127, 0.08474590629339218, 0.018472932279109955, 0.043630778789520264, 0.06053648889064789, 0.019885368645191193, 0.04718777537345886, 0.04790257662534714, 0.08448995649814606, 0.06943407654762268, 0.05400310456752777, 0.05782090872526169, 0.030959248542785645, 0.045651309192180634, 0.05860988050699234, 0.02080570161342621, 0.05053548514842987, 0.049076713621616364, 0.08277333527803421, 0.07531554996967316, 0.05549667030572891, 0.06333988159894943, 0.051450975239276886, 0.0700523629784584, 0.09300757199525833, 0.053061746060848236, 0.07821796834468842, 0.07586198300123215, 0.11389046907424927, 0.09501983970403671, 0.07218705117702484, 0.07785050570964813, 0.057597510516643524, 0.07757522910833359, 0.1031179279088974, 0.05530133843421936, 0.08568276464939117, 0.09492999315261841, 0.1300196647644043, 0.11422819644212723, 0.10058294236660004, 0.10245879739522934, 0.061026714742183685, 0.0668773502111435, 0.07391086220741272, 0.03329072892665863, 0.06537970155477524, 0.06366315484046936, 0.09696951508522034, 0.08287804573774338, 0.06653556227684021, 0.07465802878141403, 0.026167094707489014, 0.037362150847911835, 0.04695017635822296, 0.020476818084716797, 0.04997645318508148, 0.05742526799440384, 0.08659840375185013, 0.07137499004602432, 0.0533984899520874, 0.051762655377388, 0.04533527046442032, 0.06045635789632797, 0.05807771533727646, 0.02058827131986618, 0.05108091980218887, 0.05395187437534332, 0.08595738559961319, 0.08151126652956009, 0.06491593271493912, 0.07128263264894485, 0.04654025286436081, 0.07879363745450974, 0.08196096867322922, 0.03540752828121185, 0.0671842023730278, 0.07584062963724136, 0.11109472811222076, 0.0965246632695198, 0.0819978415966034, 0.08045925199985504, 0.03305468708276749, 0.06345482170581818, 0.06073198467493057, 0.022525779902935028, 0.05146114528179169, 0.05375738441944122, 0.09325883537530899, 0.08364342153072357, 0.06342456489801407, 0.07225459814071655, 0.034571439027786255, 0.054270170629024506, 0.056773267686367035, 0.022746406495571136, 0.048646196722984314, 0.035019733011722565, 0.08009526878595352, 0.06708067655563354, 0.03657951205968857, 0.06034735590219498, 0.02335197478532791, 0.0536777600646019, 0.05070025473833084, 0.014776855707168579, 0.0467366948723793, 0.049215734004974365, 0.09648510813713074, 0.0688517764210701, 0.043644748628139496, 0.04402320086956024, 0.011546790599822998, 0.053042441606521606, 0.057926490902900696, 0.031668588519096375, 0.05674058198928833, 0.05959760397672653, 0.09380792081356049, 0.07161711156368256, 0.050678856670856476, 0.04000100493431091, 0.02928014099597931, 0.05036701261997223, 0.062220364809036255, 0.021768204867839813, 0.0540698766708374, 0.06152217090129852, 0.10138861835002899, 0.08422685414552689, 0.06121151149272919, 0.05848517268896103, 0.06338314712047577, 0.08399868756532669, 0.09411584585905075, 0.05737190693616867, 0.08736182004213333, 0.09169487655162811, 0.12644898891448975, 0.11101797223091125, 0.08528528362512589, 0.08948004990816116, 0.04647589474916458, 0.0698874294757843, 0.08067487180233002, 0.03500215709209442, 0.06575191766023636, 0.07538020610809326, 0.1030704453587532, 0.09748145192861557, 0.07863897830247879, 0.08461913466453552, 0.05940684676170349, 0.071843221783638, 0.07718813419342041, 0.03920839726924896, 0.06642082333564758, 0.07210100442171097, 0.10203465819358826, 0.09492956846952438, 0.07022295147180557, 0.07109394669532776, 0.040743015706539154, 0.06581028550863266, 0.0757574811577797, 0.03502886742353439, 0.06313401460647583, 0.06489033252000809, 0.10827596485614777, 0.09550607949495316, 0.07260096073150635, 0.07755524665117264, 0.042995430529117584, 0.07043483108282089, 0.08025567978620529, 0.04368826001882553, 0.06738004833459854, 0.06128918379545212, 0.1096796989440918, 0.10664539039134979, 0.08597338944673538, 0.0979962944984436, 0.057995013892650604, 0.08900892734527588, 0.09039631485939026, 0.05332665145397186, 0.07561878859996796, 0.07291301339864731, 0.11464758217334747, 0.10035323351621628, 0.08673723042011261, 0.08679719269275665, 0.059011057019233704, 0.08378545939922333, 0.08229537308216095, 0.04639188200235367, 0.07426238805055618, 0.07483912259340286, 0.10774703323841095, 0.09941252321004868, 0.09003229439258575, 0.09781305491924286, 0.033561721444129944, 0.0556798130273819, 0.05718671530485153, 0.026711516082286835, 0.053220562636852264, 0.06254950910806656, 0.092891626060009, 0.07615453004837036, 0.05462244153022766, 0.05592840909957886, 0.040045224130153656, 0.0619206503033638, 0.05688870698213577, 0.02217690646648407, 0.050646744668483734, 0.05751682072877884, 0.09854009002447128, 0.08959007263183594, 0.07239852845668793, 0.07446033507585526, 0.06291863322257996, 0.07848948240280151, 0.08308028429746628, 0.05134366452693939, 0.07476857304573059, 0.08515464514493942, 0.11387879401445389, 0.10861848294734955, 0.08616258949041367, 0.09312763810157776, 0.05301060527563095, 0.07278522104024887, 0.08578948676586151, 0.048567697405815125, 0.07315227389335632, 0.07903040945529938, 0.11396852880716324, 0.10662498325109482, 0.08230116218328476, 0.081371009349823, 0.044062159955501556, 0.06736194342374802, 0.07732653617858887, 0.0412306934595108, 0.061816826462745667, 0.06608011573553085, 0.10446690768003464, 0.09569971263408661, 0.08236311376094818, 0.08705063164234161, 0.04928753525018692, 0.07427068054676056, 0.09539375454187393, 0.07186564803123474, 0.08422333002090454, 0.09719328582286835, 0.12899410724639893, 0.10869660973548889, 0.09619806706905365, 0.08669769763946533, 0.05385974794626236, 0.06483982503414154, 0.0811152309179306, 0.043579839169979095, 0.06836877763271332, 0.07405440509319305, 0.11594132333993912, 0.10522061586380005, 0.09160018712282181, 0.0931139886379242, 0.04514726251363754, 0.06005037575960159, 0.06474640220403671, 0.03222395479679108, 0.06003754585981369, 0.06503669172525406, 0.09668739140033722, 0.08253288269042969, 0.07129635661840439, 0.07856284081935883, 0.045126013457775116, 0.06958422809839249, 0.08518289029598236, 0.054403603076934814, 0.08229196816682816, 0.08831958472728729, 0.11490471661090851, 0.09999952465295792, 0.08962107449769974, 0.08743172883987427, 0.03704679757356644, 0.0589238703250885, 0.056747227907180786, 0.02275729924440384, 0.05268687754869461, 0.05402381718158722, 0.08972419798374176, 0.07657994329929352, 0.05823457986116409, 0.06971915811300278, 0.058491118252277374, 0.07932797819375992, 0.09235541522502899, 0.05368046462535858, 0.08471068739891052, 0.09064336121082306, 0.12023042142391205, 0.1044842079281807, 0.08578585833311081, 0.09422468394041061, 0.03664115071296692, 0.05501576513051987, 0.0582808256149292, 0.023610934615135193, 0.05746208131313324, 0.06046941131353378, 0.09727809578180313, 0.08086814731359482, 0.05216450244188309, 0.06667633354663849, 0.027212537825107574, 0.03336154669523239, 0.04778538644313812, 0.01720215380191803, 0.04322345554828644, 0.05627927929162979, 0.08363191783428192, 0.07637421786785126, 0.04782041162252426, 0.055997334420681, 0.04340306669473648, 0.05993454158306122, 0.05811447650194168, 0.02301204204559326, 0.05545505881309509, 0.05925190448760986, 0.08339326083660126, 0.08222584426403046, 0.05432474613189697, 0.067087322473526, 0.04343175143003464, 0.05176921188831329, 0.05880243331193924, 0.019609525799751282, 0.058857835829257965, 0.06913727521896362, 0.09264719486236572, 0.08110538125038147, 0.04999499022960663, 0.06114697456359863, 0.0476994514465332, 0.0676107108592987, 0.05935010313987732, 0.030316062271595, 0.05974508821964264, 0.06476742029190063, 0.09795365482568741, 0.08381377905607224, 0.06567718833684921, 0.07254192233085632, 0.044850483536720276, 0.06531882286071777, 0.0684155821800232, 0.03534894436597824, 0.06024717539548874, 0.07471513748168945, 0.10342007130384445, 0.0967632457613945, 0.0788058415055275, 0.08897244930267334, 0.03889212757349014, 0.04608318954706192, 0.0556308776140213, 0.023325704038143158, 0.051457732915878296, 0.054181575775146484, 0.08595626056194305, 0.0826256200671196, 0.05679774284362793, 0.07128200680017471, 0.031298257410526276, 0.05606251209974289, 0.06747104227542877, 0.023720741271972656, 0.0532517284154892, 0.058439500629901886, 0.09332658350467682, 0.09000212699174881, 0.06584473699331284, 0.06956268846988678, 0.04420012980699539, 0.05453591048717499, 0.050449758768081665, 0.01478874683380127, 0.04630277305841446, 0.043417274951934814, 0.08136378228664398, 0.06692111492156982, 0.048984989523887634, 0.06643597781658173, 0.055180490016937256, 0.0728827565908432, 0.07454013079404831, 0.03595258295536041, 0.07435902953147888, 0.07310228794813156, 0.10991951823234558, 0.09195838868618011, 0.06907063722610474, 0.08057556301355362, 0.027676761150360107, 0.0374189168214798, 0.04935628920793533, 0.013066470623016357, 0.041237324476242065, 0.046871885657310486, 0.07960531860589981, 0.06637182086706161, 0.047983817756175995, 0.048027411103248596, 0.04164334386587143, 0.04751148819923401, 0.06106458604335785, 0.017904072999954224, 0.05370873957872391, 0.053233399987220764, 0.09271611273288727, 0.07980071753263474, 0.05529341846704483, 0.06307341903448105, 0.025611020624637604, 0.046345748007297516, 0.050167515873909, 0.01391550898551941, 0.0427929162979126, 0.04331369698047638, 0.07996945083141327, 0.07322349399328232, 0.05337116867303848, 0.0625532865524292, 0.03612544387578964, 0.04851023107767105, 0.056751519441604614, 0.008020766079425812, 0.05092674493789673, 0.052226826548576355, 0.07609094679355621, 0.07085756957530975, 0.05429898202419281, 0.06640144437551498, 0.04686737060546875, 0.06784626096487045, 0.07720652222633362, 0.03905191272497177, 0.07224098592996597, 0.07327162474393845, 0.10364783555269241, 0.09710591286420822, 0.0809643566608429, 0.09137097001075745, 0.057107798755168915, 0.07018022984266281, 0.08665274828672409, 0.04030061513185501, 0.06865142285823822, 0.07856777310371399, 0.10143685340881348, 0.10109808295965195, 0.07182040065526962, 0.0765986368060112, 0.02843785285949707, 0.04401272535324097, 0.05414988100528717, 0.019670091569423676, 0.04567334055900574, 0.04592388868331909, 0.07103916257619858, 0.06522197276353836, 0.0461694598197937, 0.046926967799663544, 0.04507485032081604, 0.0735393613576889, 0.07581394910812378, 0.04310644418001175, 0.06718646734952927, 0.0691775530576706, 0.10594723373651505, 0.09915555268526077, 0.08385231345891953, 0.0967683270573616, 0.043513670563697815, 0.06746110320091248, 0.0729081779718399, 0.029438987374305725, 0.06475871801376343, 0.06216365098953247, 0.09463846683502197, 0.0862949788570404, 0.06668850034475327, 0.07809679210186005, 0.04828599840402603, 0.07165776193141937, 0.08449489623308182, 0.04656190425157547, 0.07947714626789093, 0.08443715423345566, 0.11680116504430771, 0.09668572247028351, 0.08259382843971252, 0.08405923843383789, 0.0585947260260582, 0.08543021231889725, 0.10098757594823837, 0.0706777572631836, 0.08983204513788223, 0.09680932015180588, 0.12421399354934692, 0.10251608490943909, 0.09867404401302338, 0.09573475271463394, 0.05157718062400818, 0.05944998562335968, 0.07127667963504791, 0.03609864413738251, 0.05777149647474289, 0.07039614766836166, 0.10156085342168808, 0.08415887504816055, 0.06215706467628479, 0.06794888526201248, 0.046646445989608765, 0.073096863925457, 0.07760114967823029, 0.048289887607097626, 0.07109246402978897, 0.06942141801118851, 0.09710869938135147, 0.0824541226029396, 0.06725239753723145, 0.0785614401102066, 0.048805274069309235, 0.07508902251720428, 0.08374030888080597, 0.051150597631931305, 0.07716492563486099, 0.07572482526302338, 0.1193232536315918, 0.10001879185438156, 0.08663530647754669, 0.09202895313501358, 0.0630415752530098, 0.08324488252401352, 0.10732860863208771, 0.07788418233394623, 0.08971330523490906, 0.09565584361553192, 0.1184360682964325, 0.10073275119066238, 0.09788986295461655, 0.09798645973205566, 0.07168108969926834, 0.092125304043293, 0.11311987042427063, 0.0815839022397995, 0.09403878450393677, 0.10274137556552887, 0.12436684221029282, 0.10883744806051254, 0.10586827993392944, 0.10412020981311798, 0.05725100636482239, 0.07062920928001404, 0.09125059843063354, 0.06435834616422653, 0.07578380405902863, 0.08356522023677826, 0.09944196790456772, 0.0807918980717659, 0.0810161679983139, 0.07679524272680283, 0.06923357397317886, 0.07871153205633163, 0.10368001461029053, 0.07835811376571655, 0.0915268212556839, 0.09526382386684418, 0.12244971096515656, 0.09822344779968262, 0.09781413525342941, 0.1006729006767273, 0.04631738364696503, 0.05975296348333359, 0.06773373484611511, 0.024824857711791992, 0.06806834787130356, 0.0640605241060257, 0.09041859209537506, 0.06973780691623688, 0.04936545342206955, 0.07134709507226944, 0.03706437349319458, 0.06993450224399567, 0.09221915155649185, 0.051733776926994324, 0.08235688507556915, 0.08600246161222458, 0.11950720846652985, 0.1012495681643486, 0.08381520956754684, 0.09398065507411957, 0.04348939657211304, 0.07320509105920792, 0.07597310841083527, 0.050105020403862, 0.0738765075802803, 0.07951685786247253, 0.11352338641881943, 0.097854845225811, 0.07931488752365112, 0.08191218972206116, 0.06627591699361801, 0.08820141851902008, 0.09961110353469849, 0.07530342042446136, 0.08964253962039948, 0.0967508852481842, 0.11612937599420547, 0.09652703255414963, 0.09353097528219223, 0.09015088528394699, 0.05709555000066757, 0.09001680463552475, 0.08479572087526321, 0.046441152691841125, 0.07656674087047577, 0.08489140123128891, 0.11210542172193527, 0.09350807964801788, 0.07672561705112457, 0.09332284331321716, 0.05247758328914642, 0.07531420886516571, 0.08929181843996048, 0.05222272127866745, 0.07989008724689484, 0.08247213065624237, 0.11051737517118454, 0.09456298500299454, 0.07548493146896362, 0.0896773487329483, 0.048978760838508606, 0.06785274296998978, 0.08889739215373993, 0.046708181500434875, 0.07678734511137009, 0.0766649916768074, 0.11872602999210358, 0.09961631894111633, 0.08172992616891861, 0.08847346901893616, 0.0644722580909729, 0.09548236429691315, 0.09390721470117569, 0.061738334596157074, 0.08251314610242844, 0.08407995849847794, 0.1127302348613739, 0.10800059139728546, 0.09370386600494385, 0.10553637146949768, 0.06591212749481201, 0.07534228265285492, 0.09727488458156586, 0.07178881764411926, 0.08045895397663116, 0.08508671075105667, 0.0999940037727356, 0.0817568227648735, 0.0842173621058464, 0.08095989376306534, 0.07943236082792282, 0.08312468230724335, 0.11101039499044418, 0.08108928799629211, 0.09849780797958374, 0.10069424659013748, 0.07290370762348175, 0.07107583433389664, 0.10226508229970932, 0.021805018186569214, 0.1209716871380806, 0.07592641562223434, 0.09711980819702148, 0.07320867478847504, 0.12165485322475433, 0.06205490231513977, 0.12941253185272217, 0.06937222927808762, 0.0736081451177597, 0.05889693647623062, 0.09530805796384811, 0.09611140936613083, 0.04355911165475845, 0.05056197941303253, 0.0945209488272667, 0.09580901265144348, 0.1102992445230484, 0.10035540908575058, 0.08871476352214813, 0.11932368576526642, 0.11646698415279388, 0.12410610169172287, 0.1142723560333252, 0.11186327040195465, 0.10540175437927246, 0.07871486246585846, 0.11278246343135834, 0.07991313189268112, 0.12259236723184586, 0.0667964443564415, 0.10843702405691147, 0.06435322761535645, 0.12266112864017487, 0.08367966115474701, 0.09015136957168579, 0.06780121475458145, 0.11087920516729355, 0.06347928196191788, 0.10242358595132828, 0.08176314830780029, 0.09440936148166656, 0.08006277680397034, 0.10210855305194855, 0.09013203531503677, 0.08133065700531006, 0.044493719935417175, 0.10073789209127426, 0.05469725281000137, 0.04843667149543762, 0.03958669304847717, 0.0741519033908844, 0.06717383861541748, 0.05361048877239227, 0.03109864890575409, 0.07090314477682114, 0.043723344802856445, 0.07266610860824585, 0.03981725871562958, 0.04391676187515259, 0.03528820723295212, 0.059173814952373505, 0.060009509325027466, 0.05274093151092529, 0.05428619682788849, 0.07809006422758102, 0.055039823055267334, 0.05925177037715912, 0.05787671357393265, 0.05271697789430618, 0.022151976823806763, 0.06311920285224915, 0.053794026374816895, 0.04422437399625778, 0.02242804318666458, 0.04619556665420532, 0.05211098492145538, 0.07234567403793335, 0.05746665596961975, 0.04788827896118164, 0.07653310149908066, 0.08023367822170258, 0.08829382807016373, 0.048331595957279205, 0.05684342235326767, 0.03595847636461258, 0.026111960411071777, 0.04784122854471207, 0.029934130609035492, 0.09026263654232025, 0.03299126774072647, 0.05818674713373184, 0.02261008322238922, 0.0628751814365387, 0.04008011519908905, 0.08218096196651459, 0.05463632196187973, 0.05451309680938721, 0.03990255296230316, 0.08933595567941666, 0.02927955985069275, 0.06065814942121506, 0.03559691458940506, 0.040409110486507416, 0.007030561566352844, 0.0786236897110939, 0.06348736584186554, 0.04888080060482025, 0.0767485722899437, 0.06590358167886734, 0.04854915291070938, 0.05057463049888611, 0.017423011362552643, 0.05009111016988754, 0.0522143617272377, 0.05870070308446884, 0.038632832467556, 0.06496717780828476, 0.03570093959569931, 0.04667220264673233, 0.023667335510253906, 0.05449601262807846, 0.01176530122756958, 0.0400332435965538, 0.011762268841266632, 0.05859605222940445, 0.04173515737056732, 0.03787101060152054, 0.09849780797958374, 0.09192189574241638, 0.07290370762348175, 0.07107583433389664, 0.07698975503444672, 0.06254776567220688, 0.021805018186569214, 0.11875531077384949, 0.07487457245588303, 0.1209716871380806, 0.07426875084638596, 0.058740243315696716, 0.055506959557533264, 0.07320867478847504, 0.12165485322475433, 0.06205493211746216, 0.08754073083400726, 0.12941253185272217, 0.06937222927808762, 0.10433296859264374, 0.06381421536207199, 0.0736081451177597, 0.053671859204769135, 0.07365491986274719, 0.11145211011171341, 0.09611140936613083, 0.0796549841761589, 0.03533980995416641, 0.056042566895484924, 0.05056197941303253, 0.07410146296024323, 0.09143844246864319, 0.0958089530467987, 0.09005685895681381, 0.07518821209669113, 0.08871476352214813, 0.11932368576526642, 0.1149122342467308, 0.12410610169172287, 0.09654782712459564, 0.1142723560333252, 0.11186327040195465, 0.0746094286441803, 0.07886845618486404, 0.0637795552611351, 0.09613620489835739, 0.07987421751022339, 0.07991320639848709, 0.08977003395557404, 0.1020243763923645, 0.09004531800746918, 0.10843702405691147, 0.06435322761535645, 0.09214373677968979, 0.1127399280667305, 0.07680008560419083, 0.08367966115474701, 0.0611083060503006, 0.05298015475273132, 0.06780121475458145, 0.11087920516729355, 0.05652250349521637, 0.08213313668966293, 0.03638487309217453, 0.08176314830780029, 0.11808371543884277, 0.07936257869005203, 0.06831424683332443, 0.07931763678789139, 0.02320704609155655, 0.09553292393684387, 0.14193478226661682, 0.09013203531503677, 0.0585145428776741, 0.06384360790252686, 0.05838078260421753, 0.044493719935417175, 0.09769180417060852, 0.088401660323143, 0.05469725281000137, 0.06269097328186035, 0.013842001557350159, 0.07019417732954025, 0.05551454424858093, 0.042056627571582794, 0.06717383861541748, 0.08536125719547272, 0.04147090017795563, 0.05361048877239227, 0.0393030121922493, 0.0659203752875328, 0.09330262988805771, 0.043723344802856445, 0.07266610860824585, 0.03981725871562958, 0.05066365748643875, 0.04391676187515259, 0.03528820723295212, 0.054385602474212646, 0.06399358808994293, 0.06491411477327347, 0.03301151841878891, 0.041306935250759125, 0.05428619682788849, 0.07809006422758102, 0.044682636857032776, 0.0504634827375412, 0.015916824340820312, 0.05787671357393265, 0.08086324483156204, 0.04178979992866516, 0.03012671321630478, 0.028482049703598022, -0.004224888980388641, 0.05053120106458664, 0.0863061249256134, 0.05379403382539749, 0.0656050518155098, 0.042612262070178986, 0.02497873455286026, 0.021279416978359222, -0.018689602613449097, 0.06146816909313202, 0.050101421773433685, 0.05211103707551956, 0.04607996344566345, 0.05746673047542572, 0.029934793710708618, 0.02675856649875641, 0.07653319835662842, 0.08762718737125397, 0.08829397708177567, 0.048181839287281036]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in train_dataloader:\n",
    "    input_ids, attention_mask, _ = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(logits.squeeze().cpu().tolist())\n",
    "\n",
    "# 예측값 출력\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c434af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c171d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
