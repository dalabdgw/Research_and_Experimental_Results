{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea2ba61",
   "metadata": {},
   "source": [
    "# 0. Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1d7d1",
   "metadata": {},
   "source": [
    "### 0_1. 데이터 정의\n",
    "- 입력 데이터는 1.csv, 2.csv, 3.csv, ... 와 같이 n.csv 형식의 파일\n",
    "    - 입력 데이터: 악보 정보 (노트, 강도, 페달 등)\n",
    "- 정답 데이터는 1_target.csv, 2_target.csv, 3_target.csv, ... 와 같이 n_target.csv 형식의 파일\n",
    "    - 정답 데이터: 각 기준별 평가 점수\n",
    "    \n",
    "### 0_2. 데이터 전처리 방법\n",
    "- input data\n",
    "    - sec: 가만히 유지\n",
    "    - msg_type: note_on, note_off 정보는 딥러닝 학습에 무의미하므로 삭제\n",
    "    - channel: 삭제\n",
    "    - note: MinMaxScaler (노트값은 데이터 분포가 정규 분포를 따르지 않을 가능성 ↑. 추가 실험 예정)\n",
    "    - velocity: StandardScaler (velocity는 일정 정규 분포를 따를 가능성 ↑. 추가 실험 예정)\n",
    "    - dynamic: One-Hot Encoding (각 강도 값을 고유한 벡터로 변환하여 모델이 더 쉽게 학습할 수 있도록 합니다.)\n",
    "    - padal: StandardScaler\n",
    "    - count: 삭제 (음악 정보와 관련성이 낮다고 판단)\n",
    "    - main_vol: 삭제 (음악 정보와 관련성이 낮다고 판단)\n",
    "    - depth: 삭제 (음악 정보와 관련성이 낮다고 판단)\n",
    "    - pan: 삭제 (음악 정보와 관련성이 낮다고 판단)\n",
    "    \n",
    "### 0_3. 모델 선택 및 이유\n",
    "- RNN & LSTM\n",
    "    - 본 연구의 경우 입력 데이터의 행의 수가 연주된 곡의 길이에 따라 전부 다름(시간 의존성 내포)\n",
    "    - 피아노 연주의 경우 이전의 연주가 다음의 연주와 이어지는 Sequence Data이므로 이를 처리하는데 적합한 RNN 모델을 선택\n",
    "    - 또한, RNN 모델의 경우 긴 시퀀스 데이터의 경우 vanishing gradient 혹은 exploding gradient 문제가 발생하기 때문에 LSTM 모델을 선택\n",
    "        - LSTM 모델은 셀 내부에 게이트 메커니즘을 사용하여 과거 정보를 유지하고 불필요한 정보를 잊도록 설계되어 이러한 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b2330",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8deb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76f454",
   "metadata": {},
   "source": [
    "# 2. Data Load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "835b9983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from input data files:\n",
      "The number of input data: 3\n",
      "[[0.0 array([1., 0.]) array([0., 1.]) 0 0 list([1.0, 0.0, 0.0, 0.0, 0.0])]\n",
      " [0.1 array([0.]) array([0.]) 0 0 list([0.0, 0.0, 0.0, 0.0, 1.0])]\n",
      " [0.2 array([0.]) array([0.]) 0 0 list([0.0, 0.0, 0.0, 0.0, 1.0])]\n",
      " [0.3 array([0.]) array([0.]) 0 0 list([0.0, 0.0, 0.0, 0.0, 1.0])]\n",
      " [0.4 array([1.        , 0.        , 0.96296296, 0.25925926])\n",
      "  array([0.        , 0.        , 0.80681818, 1.        ]) 0 0\n",
      "  list([0.0, 0.0, 1.0, 0.0, 0.0])]\n",
      " [0.5 array([0.]) array([0.]) 0 0 list([0.0, 0.0, 0.0, 0.0, 1.0])]\n",
      " [0.6 array([0.]) array([0.]) 0 0 list([0.0, 0.0, 0.0, 0.0, 1.0])]\n",
      " [0.7 array([0.]) array([0.]) 0 0 list([0.0, 0.0, 0.0, 0.0, 1.0])]\n",
      " [0.8 array([0.]) array([0.]) 0 0 list([0.0, 0.0, 0.0, 0.0, 1.0])]\n",
      " [0.9 array([0.95, 0.  , 1.  , 0.25])\n",
      "  array([0.        , 0.        , 0.87777778, 1.        ]) 0 0\n",
      "  list([0.0, 0.0, 1.0, 0.0, 0.0])]]\n",
      "(3594, 6)\n",
      "(133, 6)\n",
      "(133, 6)\n",
      "\n",
      "Sample from target data files:\n",
      "The number of input data: 3\n",
      "[45.  64.  66.  22.4 45.1 10. ]\n",
      "[13 46 44 22 17 25]\n",
      "[9 8 7 6 1 0]\n",
      "(1, 6)\n",
      "(1, 6)\n",
      "(1, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(filenames):\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "\n",
    "    scaler_note = MinMaxScaler()\n",
    "    scaler_velocity = MinMaxScaler()\n",
    "\n",
    "    for filename in filenames:\n",
    "        df = pd.read_csv('./data/input/' + filename)\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        df.drop(['msg_type', 'channel', 'count', 'main_vol', 'depth', 'pan'], axis=1, inplace=True)\n",
    "\n",
    "        # Convert 'note' and 'velocity' columns from string to list and handle empty strings\n",
    "        df['note'] = df['note'].apply(lambda x: [int(i) for i in x.strip('[]').split(', ') if i != ''])\n",
    "        df['velocity'] = df['velocity'].apply(lambda x: [int(i) for i in x.strip('[]').split(', ') if i != ''])\n",
    "\n",
    "        # Replace empty lists with [0] for 'note' and 'velocity' columns\n",
    "        df['note'] = df['note'].apply(lambda x: x if x else [0])\n",
    "        df['velocity'] = df['velocity'].apply(lambda x: x if x else [0])\n",
    "\n",
    "        # Normalize 'note' and 'velocity' columns\n",
    "        df['note'] = df['note'].apply(lambda x: scaler_note.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
    "        df['velocity'] = df['velocity'].apply(lambda x: scaler_velocity.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
    "\n",
    "        # One-hot encode 'dynamic' column if it exists\n",
    "        if 'dynamic' in df.columns:\n",
    "            encoder = OneHotEncoder(sparse=False, categories='auto', drop=None, handle_unknown='ignore')\n",
    "            encoded_dynamic = encoder.fit_transform(df[['dynamic']])\n",
    "            # Convert one-hot encoding to NumPy array\n",
    "            encoded_dynamic_array = np.array(encoded_dynamic)\n",
    "            df.drop(['dynamic'], axis=1, inplace=True)\n",
    "            df['dynamic'] = encoded_dynamic_array.tolist()\n",
    "\n",
    "        # Convert dataframe to numpy array\n",
    "        data = df.to_numpy()\n",
    "\n",
    "        # Append data for each file\n",
    "        input_data.append(data)\n",
    "\n",
    "        # Load target data\n",
    "        target_filename = filename.replace('.csv', '_target.csv')\n",
    "        df_target = pd.read_csv('./data/target/' + target_filename)\n",
    "        target_data.append(df_target.to_numpy())\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "filenames = ['1.csv', '2.csv', '3.csv'] # 나중에 반복문으로 자동화\n",
    "\n",
    "input_data, target_data = load_data(filenames)\n",
    "\n",
    "print('Sample from input data files:')\n",
    "print(f'The number of input data: {len(input_data)}')\n",
    "print(input_data[0][:10])\n",
    "print(input_data[0].shape)\n",
    "print(input_data[1].shape)\n",
    "print(input_data[2].shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Sample from target data files:')\n",
    "print(f'The number of input data: {len(input_data)}')\n",
    "print(target_data[0][0])\n",
    "print(target_data[1][0])\n",
    "print(target_data[2][0])\n",
    "print(target_data[0].shape)\n",
    "print(target_data[1].shape)\n",
    "print(target_data[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70b3d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data/train/input\", exist_ok=True)\n",
    "os.makedirs(\"./data/train/target\", exist_ok=True)\n",
    "os.makedirs(\"./data/val/input\", exist_ok=True)\n",
    "os.makedirs(\"./data/val/target\", exist_ok=True)\n",
    "os.makedirs(\"./data/test/input\", exist_ok=True)\n",
    "os.makedirs(\"./data/test/target\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4fee7df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.csv', '2.csv', '3.csv']\n",
      "['1_target.csv', '2_target.csv', '3_target.csv']\n",
      "2.csv이 train 폴더로 이동, target 파일 동일\n",
      "1.csv이 train 폴더로 이동, target 파일 동일\n",
      "3.csv이 test 폴더로 이동, target 파일 동일\n"
     ]
    }
   ],
   "source": [
    "input_data_dir = \"./data/input\"\n",
    "target_data_dir = \"./data/target\"\n",
    "input_filenames = os.listdir(input_data_dir)\n",
    "\n",
    "# target_filenames 생성\n",
    "target_filenames = [filename.replace('.csv', '_target.csv') for filename in input_filenames]\n",
    "\n",
    "print(input_filenames)\n",
    "print(target_filenames)\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "random.shuffle(input_filenames)\n",
    "\n",
    "train_filenames = input_filenames[:int(len(input_filenames) * train_ratio)]\n",
    "val_filenames = input_filenames[int(len(input_filenames) * train_ratio):int(len(input_filenames) * (train_ratio + val_ratio))]\n",
    "test_filenames = input_filenames[int(len(input_filenames) * (train_ratio + val_ratio)):]\n",
    "\n",
    "for filename in train_filenames:\n",
    "    input_filepath = os.path.join(input_data_dir, filename)\n",
    "    target_filepath = os.path.join(target_data_dir, target_filenames[input_filenames.index(filename)])\n",
    "    if os.path.exists(target_filepath):  # 대응되는 타겟 파일이 존재하는 경우에만 이동\n",
    "        os.rename(input_filepath, os.path.join(\"./data/train/input\", filename))\n",
    "        os.rename(target_filepath, os.path.join(\"./data/train/target\", target_filenames[input_filenames.index(filename)]))\n",
    "        print(f'{filename}이 train 폴더로 이동, target 파일 동일')\n",
    "\n",
    "for filename in val_filenames:\n",
    "    input_filepath = os.path.join(input_data_dir, filename)\n",
    "    target_filepath = os.path.join(target_data_dir, target_filenames[input_filenames.index(filename)])\n",
    "    if os.path.exists(target_filepath):  # 대응되는 타겟 파일이 존재하는 경우에만 이동\n",
    "        os.rename(input_filepath, os.path.join(\"./data/val/input\", filename))\n",
    "        os.rename(target_filepath, os.path.join(\"./data/val/target\", target_filenames[input_filenames.index(filename)]))\n",
    "        print(f'{filename}이 val 폴더로 이동, target 파일 동일')\n",
    "\n",
    "for filename in test_filenames:\n",
    "    input_filepath = os.path.join(input_data_dir, filename)\n",
    "    target_filepath = os.path.join(target_data_dir, target_filenames[input_filenames.index(filename)])\n",
    "    if os.path.exists(target_filepath):  # 대응되는 타겟 파일이 존재하는 경우에만 이동\n",
    "        os.rename(input_filepath, os.path.join(\"./data/test/input\", filename))\n",
    "        os.rename(target_filepath, os.path.join(\"./data/test/target\", target_filenames[input_filenames.index(filename)]))\n",
    "        print(f'{filename}이 test 폴더로 이동, target 파일 동일')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd2013ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets (80% train, 20% test)\n",
    "filenames = ['1.csv', '2.csv', '3.csv']  # Replace with actual filenames\n",
    "\n",
    "np.random.shuffle(filenames)\n",
    "\n",
    "split_index = int(0.8 * len(filenames))\n",
    "\n",
    "# Training filenames\n",
    "train_filenames = filenames[:split_index]\n",
    "\n",
    "# Test filenames\n",
    "test_filenames = filenames[split_index:]\n",
    "\n",
    "# Load training data\n",
    "x_train, y_train = load_data(train_filenames)\n",
    "\n",
    "# Load test data\n",
    "x_test, y_test = load_data(test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f78dde",
   "metadata": {},
   "source": [
    "# 3. Implement Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08211bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = x_data.shape[1]\n",
    "num_outputs = y_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790393b9",
   "metadata": {},
   "source": [
    "### 3_1. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3695c69b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         768       \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, None, 128)         32896     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               32896     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67334 (263.02 KB)\n",
      "Trainable params: 67334 (263.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_features, output_dim=128),\n",
    "    # Use the SimpleRNN layer for a basic RNN\n",
    "    SimpleRNN(128, activation='relu', return_sequences=True),\n",
    "    SimpleRNN(128, activation='relu'),\n",
    "    Dense(num_outputs)\n",
    "])\n",
    "\n",
    "# 2. Summarization\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7666af42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[206], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3. Train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# 3. Train the model\n",
    "model.compile(loss = 'mse', optimizer = 'adam', metrics = ['mae'])\n",
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "95a41d44",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Evaluate the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss, mae \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m'\u001b[39m, loss)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE:\u001b[39m\u001b[38;5;124m'\u001b[39m, mae)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# 4. Evaluate the model\n",
    "loss, mae = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a51aa",
   "metadata": {},
   "source": [
    "### 3_2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2d6a9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_17 (Embedding)    (None, None, 128)         1536      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 128)         131584    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 265478 (1.01 MB)\n",
      "Trainable params: 265478 (1.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_features, output_dim=128),\n",
    "    LSTM(128, activation='relu', return_sequences=True),\n",
    "    LSTM(128, activation='relu'),\n",
    "    Dense(num_outputs)\n",
    "])\n",
    "\n",
    "# 2. Summarization\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "73e369a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3. Train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. Train the model\n",
    "model.compile(loss = 'mse', optimizer = 'adam', metrics = ['mae'])\n",
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93420259",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Evaluate the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss, mae \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mx_test\u001b[49m, y_test)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m'\u001b[39m, loss)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE:\u001b[39m\u001b[38;5;124m'\u001b[39m, mae)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Evaluate the model\n",
    "loss, mae = model.evaluate(x_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57de39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
